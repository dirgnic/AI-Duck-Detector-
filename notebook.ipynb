{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# This is a sample Jupyter Notebook\n",
    "\n",
    "Below is an example of a code cell. \n",
    "Put your cursor into the cell and press Shift+Enter to execute it and select the next one, or click 'Run Cell' button.\n",
    "\n",
    "Press Double Shift to search everywhere for classes, files, tool windows, actions, and settings.\n",
    "\n",
    "To learn more about Jupyter Notebooks in PyCharm, see [help](https://www.jetbrains.com/help/pycharm/ipython-notebook-support.html).\n",
    "For an overview of PyCharm, go to Help -> Learn IDE features or refer to [our documentation](https://www.jetbrains.com/help/pycharm/getting-started.html)."
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "125a5a7c6a134bad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T22:47:54.466929Z",
     "start_time": "2025-03-29T22:47:54.453043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "from torchvision import transforms\n",
    "\n",
    "class DuckDataset(Dataset):\n",
    "    def __init__(self, annotations, img_dir, transform=None):\n",
    "        self.annotations = annotations\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.annotations.iloc[idx]\n",
    "        img_path = os.path.join(self.img_dir, f\"{row['DatapointID']}.png\")\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        bbox = [int(x) for x in row['BoundingBox'].split()]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label = torch.tensor([row['DuckOrNoDuck']], dtype=torch.float32)\n",
    "        bbox = torch.tensor(bbox, dtype=torch.float32)\n",
    "\n",
    "        return image, label, bbox\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "annotations = pd.read_csv('./train_dataset/train-data.csv')\n",
    "dataset = DuckDataset(annotations, './train_dataset/train_dataset/', transform=transform)\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle=True)\n"
   ],
   "id": "f9e841557ce48856",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T22:47:57.873347Z",
     "start_time": "2025-03-29T22:47:57.730802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class DuckDetector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DuckDetector, self).__init__()\n",
    "\n",
    "        # Pre-trained CNN feature extractor\n",
    "        self.base_model = models.resnet18(pretrained=True)\n",
    "        num_features = self.base_model.fc.in_features\n",
    "        self.base_model.fc = nn.Identity()\n",
    "\n",
    "        # Classification head (Duck or No Duck)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(num_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Bounding box regression head (x1, y1, x2, y2)\n",
    "        self.bbox_regressor = nn.Sequential(\n",
    "            nn.Linear(num_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 4)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.base_model(x)\n",
    "        classification = self.classifier(features)\n",
    "        bbox = self.bbox_regressor(features)\n",
    "        return classification, bbox\n",
    "\n",
    "model = DuckDetector().to(device)\n"
   ],
   "id": "8d9ef82604f36ab",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ingridcorobana/PyCharmMiscProject/.venv/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/ingridcorobana/PyCharmMiscProject/.venv/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T23:20:16.734919Z",
     "start_time": "2025-03-29T22:48:01.960171Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion_class = nn.BCELoss()\n",
    "criterion_bbox = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "epochs = 20\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for images, labels, bboxes in loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        bboxes = bboxes.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs_class, outputs_bbox = model(images)\n",
    "\n",
    "        loss_class = criterion_class(outputs_class, labels)\n",
    "\n",
    "        # Only calculate bbox loss if duck exists (label == 1)\n",
    "        bbox_mask = labels.squeeze() == 1\n",
    "        if bbox_mask.any():\n",
    "            loss_bbox = criterion_bbox(outputs_bbox[bbox_mask], bboxes[bbox_mask])\n",
    "        else:\n",
    "            loss_bbox = torch.tensor(0.0).to(device)\n",
    "\n",
    "        # Combine losses clearly\n",
    "        loss = loss_class + loss_bbox\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n"
   ],
   "id": "55cfa0f6457fdd3f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 54115.3501\n",
      "Epoch [2/20], Loss: 42444.3839\n",
      "Epoch [3/20], Loss: 25055.9872\n",
      "Epoch [4/20], Loss: 14013.5585\n",
      "Epoch [5/20], Loss: 9196.0190\n",
      "Epoch [6/20], Loss: 7572.9933\n",
      "Epoch [7/20], Loss: 6724.6425\n",
      "Epoch [8/20], Loss: 6685.1042\n",
      "Epoch [9/20], Loss: 6398.6897\n",
      "Epoch [10/20], Loss: 6310.6239\n",
      "Epoch [11/20], Loss: 6005.8485\n",
      "Epoch [12/20], Loss: 6402.2382\n",
      "Epoch [13/20], Loss: 5785.2452\n",
      "Epoch [14/20], Loss: 5827.6386\n",
      "Epoch [15/20], Loss: 5447.7969\n",
      "Epoch [16/20], Loss: 5274.1383\n",
      "Epoch [17/20], Loss: 5010.1621\n",
      "Epoch [18/20], Loss: 4411.1119\n",
      "Epoch [19/20], Loss: 3701.7318\n",
      "Epoch [20/20], Loss: 3269.1962\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T23:34:00.420943Z",
     "start_time": "2025-03-29T23:34:00.345796Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(model.state_dict(), 'duck_detector_epoch20.pth')\n",
   "id": "9afa5c9a8bc97510",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T23:50:22.936227Z",
     "start_time": "2025-03-29T23:34:36.378383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# First, initialize the model architecture\n",
    "model = DuckDetector().to(device)\n",
    "\n",
    "# Load your previously saved weights\n",
    "model.load_state_dict(torch.load('duck_detector_epoch20.pth', map_location=device))\n",
    "\n",
    "# Now, continue training normally\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)  # make sure optimizer is defined again\n",
    "criterion_class = nn.BCELoss()\n",
    "criterion_bbox = nn.MSELoss()\n",
    "\n",
    "model.train()\n",
    "additional_epochs = 10  # clearly define how many more epochs you want\n",
    "\n",
    "for epoch in range(additional_epochs):\n",
    "    total_loss = 0\n",
    "    for images, labels, bboxes in loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        bboxes = bboxes.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs_class, outputs_bbox = model(images)\n",
    "\n",
    "        loss_class = criterion_class(outputs_class, labels)\n",
    "\n",
    "        bbox_mask = labels.squeeze() == 1\n",
    "        if bbox_mask.any():\n",
    "            loss_bbox = criterion_bbox(outputs_bbox[bbox_mask], bboxes[bbox_mask])\n",
    "        else:\n",
    "            loss_bbox = torch.tensor(0.0).to(device)\n",
    "\n",
    "        loss = loss_class + loss_bbox\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    print(f\"Continued Training Epoch [{epoch+1}/{additional_epochs}], Loss: {avg_loss:.4f}\")\n"
   ],
   "id": "9fe91b6ce48664ec",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ingridcorobana/PyCharmMiscProject/.venv/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/ingridcorobana/PyCharmMiscProject/.venv/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continued Training Epoch [1/10], Loss: 3007.1113\n",
      "Continued Training Epoch [2/10], Loss: 2651.5058\n",
      "Continued Training Epoch [3/10], Loss: 2465.0838\n",
      "Continued Training Epoch [4/10], Loss: 2459.6363\n",
      "Continued Training Epoch [5/10], Loss: 2241.5328\n",
      "Continued Training Epoch [6/10], Loss: 2112.7547\n",
      "Continued Training Epoch [7/10], Loss: 2006.4342\n",
      "Continued Training Epoch [8/10], Loss: 2015.6352\n",
      "Continued Training Epoch [9/10], Loss: 1966.5223\n",
      "Continued Training Epoch [10/10], Loss: 1901.3163\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T23:53:13.783170Z",
     "start_time": "2025-03-29T23:53:13.708352Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(model.state_dict(), 'duck_detector_epoch30.pth')\n",
   "id": "87e7337de8f54f60",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T00:12:23.125599Z",
     "start_time": "2025-03-30T00:12:16.154831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "test_images_path = './test_dataset/test_dataset/'\n",
    "test_files = sorted(os.listdir(test_images_path), key=lambda x: int(x.split('.')[0]))\n",
    "\n",
    "results = []\n",
    "model.eval()\n",
    "\n",
    "for file_name in test_files:\n",
    "    image_id = int(file_name.split('.')[0])\n",
    "    img_path = os.path.join(test_images_path, file_name)\n",
    "\n",
    "    image = cv2.imread(img_path)\n",
    "    img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    img_tensor = transform(img_rgb).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_class, pred_bbox = model(img_tensor)\n",
    "\n",
    "    pred_class = pred_class.item()\n",
    "    pred_bbox = pred_bbox.cpu().numpy().flatten().astype(int)\n",
    "\n",
    "    duck_present = 1 if pred_class > 0.5 else 0\n",
    "\n",
    "    if duck_present:\n",
    "        # Get bbox coordinates clearly\n",
    "        x1, y1, x2, y2 = pred_bbox\n",
    "        bbox = f\"{x1} {y1} {x2} {y2}\"\n",
    "\n",
    "        # Extract the predicted bbox area from original image\n",
    "        bbox_img = image[y1:y2, x1:x2]\n",
    "        gray_bbox_img = cv2.cvtColor(bbox_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Threshold to count actual black pixels (duck pixels clearly)\n",
    "        _, thresh = cv2.threshold(gray_bbox_img, 10, 255, cv2.THRESH_BINARY_INV)\n",
    "        pixel_count = cv2.countNonZero(thresh)\n",
    "    else:\n",
    "        bbox = \"0 0 0 0\"\n",
    "        pixel_count = 0\n",
    "\n",
    "    results.append([image_id, duck_present, pixel_count, bbox])\n",
    "\n",
    "# Save accurate results\n",
    "submission_df = pd.DataFrame(results, columns=['DatapointID', 'DuckOrNoDuck', 'PixelCount', 'BoundingBox'])\n",
    "submission_df.to_csv('submission.csv', index=False)\n"
   ],
   "id": "d86473d9efce22ad",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T00:19:43.982022Z",
     "start_time": "2025-03-30T00:19:36.987997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Let's assume you calculated this previously:\n",
    "black_pixel_ratio = 0.65  # replace with your actual calculated ratio!\n",
    "\n",
    "test_images_path = './test_dataset/test_dataset/'\n",
    "test_files = sorted(os.listdir(test_images_path), key=lambda x: int(x.split('.')[0]))\n",
    "\n",
    "results = []\n",
    "model.eval()\n",
    "\n",
    "for file_name in test_files:\n",
    "    image_id = int(file_name.split('.')[0])\n",
    "    img_path = os.path.join(test_images_path, file_name)\n",
    "\n",
    "    image = cv2.imread(img_path)\n",
    "    img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    img_tensor = transform(img_rgb).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_class, pred_bbox = model(img_tensor)\n",
    "\n",
    "    pred_class = pred_class.item()\n",
    "    pred_bbox = pred_bbox.cpu().numpy().flatten().astype(int)\n",
    "\n",
    "    duck_present = 1 if pred_class > 0.5 else 0\n",
    "\n",
    "    if duck_present:\n",
    "        # Predicted bbox\n",
    "        x1, y1, x2, y2 = pred_bbox\n",
    "        bbox = f\"{x1} {y1} {x2} {y2}\"\n",
    "\n",
    "        # Predicted bbox area\n",
    "        pred_area = (x2 - x1) * (y2 - y1)\n",
    "\n",
    "        # Extract bbox area and count black pixels\n",
    "        bbox_img = image[y1:y2, x1:x2]\n",
    "        gray_bbox_img = cv2.cvtColor(bbox_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        _, thresh = cv2.threshold(gray_bbox_img, 10, 255, cv2.THRESH_BINARY_INV)\n",
    "        black_pixels = cv2.countNonZero(thresh)\n",
    "\n",
    "        # Maximum black pixels according to original ratio\n",
    "        max_black_pixels = int(pred_area * black_pixel_ratio)\n",
    "\n",
    "        # Adjust pixel count clearly\n",
    "        if black_pixels > max_black_pixels:\n",
    "            pixel_count = max_black_pixels\n",
    "        else:\n",
    "            pixel_count = black_pixels\n",
    "\n",
    "    else:\n",
    "        bbox = \"0 0 0 0\"\n",
    "        pixel_count = 0\n",
    "\n",
    "    results.append([image_id, duck_present, pixel_count, bbox])\n",
    "\n",
    "# Save your final accurate results\n",
    "submission_df = pd.DataFrame(results, columns=['DatapointID', 'DuckOrNoDuck', 'PixelCount', 'BoundingBox'])\n",
    "submission_df.to_csv('submission.csv', index=False)\n"
   ],
   "id": "3fa2c010aa10fc72",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T01:44:05.208238Z",
     "start_time": "2025-03-30T01:43:41.656729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_images_path = './train_dataset/train_dataset/'\n",
    "train_files = sorted(\n",
    "    [x for x in os.listdir(train_images_path) if x.split('.')[0].isdigit()],\n",
    "    key=lambda x: int(x.split('.')[0])\n",
    ")\n",
    "\n",
    "initial_pixel_preds = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for file_name in train_files:\n",
    "    img_path = os.path.join(train_images_path, file_name)\n",
    "    image = cv2.imread(img_path)\n",
    "    img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    img_tensor = transform(img_rgb).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_class, pred_bbox = model(img_tensor)\n",
    "\n",
    "    pred_bbox = pred_bbox.cpu().numpy().flatten().astype(int)\n",
    "    x1, y1, x2, y2 = pred_bbox\n",
    "\n",
    "    bbox_img = image[y1:y2, x1:x2]\n",
    "    if bbox_img.size == 0:\n",
    "        bbox_img = np.zeros((32,32,3), dtype=np.uint8)\n",
    "\n",
    "    gray_bbox_img = cv2.cvtColor(bbox_img, cv2.COLOR_BGR2GRAY)\n",
    "    _, thresh = cv2.threshold(gray_bbox_img, 10, 255, cv2.THRESH_BINARY_INV)\n",
    "    pixel_count = cv2.countNonZero(thresh)\n",
    "\n",
    "    initial_pixel_preds.append(pixel_count)\n",
    "\n",
    "print(len(initial_pixel_preds))"
   ],
   "id": "7a3035c8451209dc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1828\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T00:39:01.084481Z",
     "start_time": "2025-03-30T00:39:01.075718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "annotations = pd.read_csv('./train_dataset/train-data.csv')\n",
    "\n",
    "assert len(initial_pixel_preds) == len(annotations), \"Predictions and annotations must match!\"\n",
    "\n",
    "refiner_dataset = PixelCountRefinerDataset(\n",
    "    annotations,\n",
    "    './train_dataset/train_dataset/',\n",
    "    initial_pixel_preds,\n",
    "    transform=transform_refiner\n",
    ")\n",
    "refiner_loader = DataLoader(refiner_dataset, batch_size=16, shuffle=True)\n"
   ],
   "id": "1c6347811ce21347",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T00:26:09.150258Z",
     "start_time": "2025-03-30T00:26:09.144069Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PixelCountRefinerDataset(Dataset):\n",
    "    def __init__(self, annotations, img_dir, initial_predictions, transform=None):\n",
    "        self.annotations = annotations\n",
    "        self.img_dir = img_dir\n",
    "        self.initial_predictions = initial_predictions\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.annotations.iloc[idx]\n",
    "        img_id = row['DatapointID']\n",
    "        bbox = [int(x) for x in row['BoundingBox'].split()]\n",
    "        gt_pixel_count = row['PixelCount']\n",
    "\n",
    "        img_path = os.path.join(self.img_dir, f\"{img_id}.png\")\n",
    "        image = cv2.imread(img_path)\n",
    "\n",
    "        # Crop predicted bbox region\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        cropped_img = image[y1:y2, x1:x2]\n",
    "\n",
    "        if cropped_img.size == 0:  # Avoid empty crops\n",
    "            cropped_img = np.zeros((32,32,3), dtype=np.uint8)\n",
    "\n",
    "        if self.transform:\n",
    "            cropped_img = self.transform(cropped_img)\n",
    "\n",
    "        # Your initial pixel prediction (from first model)\n",
    "        initial_pred_pixel = self.initial_predictions[idx]\n",
    "\n",
    "        return cropped_img, torch.tensor([initial_pred_pixel], dtype=torch.float32), torch.tensor([gt_pixel_count], dtype=torch.float32)\n",
    "\n",
    "transform_refiner = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load your initial predictions (from previous inference):\n",
    "initial_pixel_preds = submission_df['PixelCount'].values\n",
    "refiner_dataset = PixelCountRefinerDataset(annotations, './train_dataset/train_dataset/', initial_pixel_preds, transform=transform_refiner)\n",
    "refiner_loader = DataLoader(refiner_dataset, batch_size=16, shuffle=True)\n"
   ],
   "id": "804319aa16e70eaa",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T00:26:12.895886Z",
     "start_time": "2025-03-30T00:26:12.891285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PixelRefiner(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PixelRefiner, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((4,4))\n",
    "        )\n",
    "        self.fc_attention = nn.Sequential(\n",
    "            nn.Linear(32*4*4 + 1, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, img, initial_pixel_count):\n",
    "        features = self.conv_layers(img)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        combined = torch.cat([features, initial_pixel_count], dim=1)\n",
    "        refined_pixel_count = self.fc_attention(combined)\n",
    "        return refined_pixel_count\n"
   ],
   "id": "af484b0cc11f7498",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T00:26:15.832122Z",
     "start_time": "2025-03-30T00:26:15.828221Z"
    }
   },
   "cell_type": "code",
   "source": "refiner_model = PixelRefiner().to(device)\n",
   "id": "848519d1da92fa41",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T00:39:59.659660Z",
     "start_time": "2025-03-30T00:39:29.203449Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optimizer_refiner = torch.optim.Adam(refiner_model.parameters(), lr=1e-4)\n",
    "criterion_refiner = nn.MSELoss()\n",
    "\n",
    "\n",
    "epochs_refiner = 5\n",
    "refiner_model.train()\n",
    "\n",
    "for epoch in range(epochs_refiner):\n",
    "    total_loss = 0\n",
    "    for imgs, initial_preds, gt_pixels in refiner_loader:\n",
    "        imgs, initial_preds, gt_pixels = imgs.to(device), initial_preds.to(device), gt_pixels.to(device)\n",
    "\n",
    "        optimizer_refiner.zero_grad()\n",
    "        refined_output = refiner_model(imgs, initial_preds)\n",
    "        loss = criterion_refiner(refined_output, gt_pixels)\n",
    "        loss.backward()\n",
    "        optimizer_refiner.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(refiner_loader)\n",
    "    print(f\"Pixel Refiner Epoch [{epoch+1}/{epochs_refiner}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "torch.save(refiner_model.state_dict(), 'pixel_refiner.pth')\n"
   ],
   "id": "a00896aeb8598549",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel Refiner Epoch [1/5], Loss: 6023571.5272\n",
      "Pixel Refiner Epoch [2/5], Loss: 5759808.7717\n",
      "Pixel Refiner Epoch [3/5], Loss: 4607347.8870\n",
      "Pixel Refiner Epoch [4/5], Loss: 2644551.9228\n",
      "Pixel Refiner Epoch [5/5], Loss: 1697470.3673\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T00:52:03.946324Z",
     "start_time": "2025-03-30T00:52:03.930352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "refiner_model = PixelRefiner().to(device)\n",
    "refiner_model.load_state_dict(torch.load('pixel_refiner.pth', map_location=device))\n"
   ],
   "id": "5c4e7c3e58fe5629",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T00:52:05.180478Z",
     "start_time": "2025-03-30T00:52:05.177657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optimizer_refiner = torch.optim.Adam(refiner_model.parameters(), lr=1e-4)\n",
    "criterion_refiner = nn.MSELoss()\n"
   ],
   "id": "d0088a09883ee5ef",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T00:53:07.662652Z",
     "start_time": "2025-03-30T00:52:07.239718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "additional_epochs = 10  # Add how many more epochs you want\n",
    "\n",
    "refiner_model.train()\n",
    "\n",
    "for epoch in range(additional_epochs):\n",
    "    total_loss = 0\n",
    "    for imgs, initial_preds, gt_pixels in refiner_loader:\n",
    "        imgs, initial_preds, gt_pixels = imgs.to(device), initial_preds.to(device), gt_pixels.to(device)\n",
    "\n",
    "        optimizer_refiner.zero_grad()\n",
    "        refined_output = refiner_model(imgs, initial_preds)\n",
    "        loss = criterion_refiner(refined_output, gt_pixels)\n",
    "        loss.backward()\n",
    "        optimizer_refiner.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(refiner_loader)\n",
    "    print(f\"Continued Pixel Refiner Epoch [{epoch+1}/{additional_epochs}], Loss: {avg_loss:.4f}\")\n"
   ],
   "id": "8c4a3e6fd5da8abe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continued Pixel Refiner Epoch [1/10], Loss: 1617895.6560\n",
      "Continued Pixel Refiner Epoch [2/10], Loss: 1573489.3372\n",
      "Continued Pixel Refiner Epoch [3/10], Loss: 1543149.4772\n",
      "Continued Pixel Refiner Epoch [4/10], Loss: 1522792.0071\n",
      "Continued Pixel Refiner Epoch [5/10], Loss: 1492897.9394\n",
      "Continued Pixel Refiner Epoch [6/10], Loss: 1468624.6633\n",
      "Continued Pixel Refiner Epoch [7/10], Loss: 1444724.7141\n",
      "Continued Pixel Refiner Epoch [8/10], Loss: 1422006.9397\n",
      "Continued Pixel Refiner Epoch [9/10], Loss: 1418509.6552\n",
      "Continued Pixel Refiner Epoch [10/10], Loss: 1391945.7663\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T00:57:50.208156Z",
     "start_time": "2025-03-30T00:57:50.204136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.save(refiner_model.state_dict(), 'pixel_refiner_continued.pth')\n",
    "print(\"Refiner model saved after additional training epochs!\")\n"
   ],
   "id": "becf4cff22021774",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refiner model saved after additional training epochs!\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9f352d71ad8862ba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T00:57:15.887500Z",
     "start_time": "2025-03-30T00:54:07.924639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "additional_epochs = 30  # Add how many more epochs you want\n",
    "\n",
    "refiner_model.train()\n",
    "\n",
    "for epoch in range(additional_epochs):\n",
    "    total_loss = 0\n",
    "    for imgs, initial_preds, gt_pixels in refiner_loader:\n",
    "        imgs, initial_preds, gt_pixels = imgs.to(device), initial_preds.to(device), gt_pixels.to(device)\n",
    "\n",
    "        optimizer_refiner.zero_grad()\n",
    "        refined_output = refiner_model(imgs, initial_preds)\n",
    "        loss = criterion_refiner(refined_output, gt_pixels)\n",
    "        loss.backward()\n",
    "        optimizer_refiner.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(refiner_loader)\n",
    "    print(f\"Continued Pixel Refiner Epoch [{epoch+1}/{additional_epochs}], Loss: {avg_loss:.4f}\")"
   ],
   "id": "ebd824e071d6e827",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continued Pixel Refiner Epoch [1/30], Loss: 1399521.1486\n",
      "Continued Pixel Refiner Epoch [2/30], Loss: 1360890.9394\n",
      "Continued Pixel Refiner Epoch [3/30], Loss: 1347520.1874\n",
      "Continued Pixel Refiner Epoch [4/30], Loss: 1334145.2549\n",
      "Continued Pixel Refiner Epoch [5/30], Loss: 1327862.4568\n",
      "Continued Pixel Refiner Epoch [6/30], Loss: 1310872.1761\n",
      "Continued Pixel Refiner Epoch [7/30], Loss: 1293566.1733\n",
      "Continued Pixel Refiner Epoch [8/30], Loss: 1294373.3609\n",
      "Continued Pixel Refiner Epoch [9/30], Loss: 1271657.3269\n",
      "Continued Pixel Refiner Epoch [10/30], Loss: 1273715.5709\n",
      "Continued Pixel Refiner Epoch [11/30], Loss: 1291761.3894\n",
      "Continued Pixel Refiner Epoch [12/30], Loss: 1253799.0845\n",
      "Continued Pixel Refiner Epoch [13/30], Loss: 1245438.2226\n",
      "Continued Pixel Refiner Epoch [14/30], Loss: 1221236.7787\n",
      "Continued Pixel Refiner Epoch [15/30], Loss: 1211577.8880\n",
      "Continued Pixel Refiner Epoch [16/30], Loss: 1226701.7560\n",
      "Continued Pixel Refiner Epoch [17/30], Loss: 1190583.3511\n",
      "Continued Pixel Refiner Epoch [18/30], Loss: 1185719.5261\n",
      "Continued Pixel Refiner Epoch [19/30], Loss: 1174867.3486\n",
      "Continued Pixel Refiner Epoch [20/30], Loss: 1165021.8319\n",
      "Continued Pixel Refiner Epoch [21/30], Loss: 1208495.8231\n",
      "Continued Pixel Refiner Epoch [22/30], Loss: 1144222.1364\n",
      "Continued Pixel Refiner Epoch [23/30], Loss: 1138563.4921\n",
      "Continued Pixel Refiner Epoch [24/30], Loss: 1134010.7340\n",
      "Continued Pixel Refiner Epoch [25/30], Loss: 1117468.7516\n",
      "Continued Pixel Refiner Epoch [26/30], Loss: 1106608.3507\n",
      "Continued Pixel Refiner Epoch [27/30], Loss: 1099588.5905\n",
      "Continued Pixel Refiner Epoch [28/30], Loss: 1097004.1143\n",
      "Continued Pixel Refiner Epoch [29/30], Loss: 1085683.4899\n",
      "Continued Pixel Refiner Epoch [30/30], Loss: 1076308.8380\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T00:58:06.626102Z",
     "start_time": "2025-03-30T00:58:06.622468Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.save(refiner_model.state_dict(), 'pixel_refiner_continued.pth')\n",
    "print(\"Refiner model saved after additional training epochs!\")"
   ],
   "id": "b824b042c770a165",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refiner model saved after additional training epochs!\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T01:56:31.624010Z",
     "start_time": "2025-03-30T01:53:29.285911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "additional_epochs = 30  # Add how many more epochs you want\n",
    "\n",
    "refiner_model.train()\n",
    "\n",
    "for epoch in range(additional_epochs):\n",
    "    total_loss = 0\n",
    "    for imgs, initial_preds, gt_pixels in refiner_loader:\n",
    "        imgs, initial_preds, gt_pixels = imgs.to(device), initial_preds.to(device), gt_pixels.to(device)\n",
    "\n",
    "        optimizer_refiner.zero_grad()\n",
    "        refined_output = refiner_model(imgs, initial_preds)\n",
    "        loss = criterion_refiner(refined_output, gt_pixels)\n",
    "        loss.backward()\n",
    "        optimizer_refiner.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(refiner_loader)\n",
    "    print(f\"Continued Pixel Refiner Epoch [{epoch+1}/{additional_epochs}], Loss: {avg_loss:.4f}\")"
   ],
   "id": "81619301ea005f13",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continued Pixel Refiner Epoch [1/30], Loss: 890307.1343\n",
      "Continued Pixel Refiner Epoch [2/30], Loss: 887078.9160\n",
      "Continued Pixel Refiner Epoch [3/30], Loss: 892068.8313\n",
      "Continued Pixel Refiner Epoch [4/30], Loss: 881282.6845\n",
      "Continued Pixel Refiner Epoch [5/30], Loss: 882212.5920\n",
      "Continued Pixel Refiner Epoch [6/30], Loss: 881468.0709\n",
      "Continued Pixel Refiner Epoch [7/30], Loss: 879330.4565\n",
      "Continued Pixel Refiner Epoch [8/30], Loss: 883346.9113\n",
      "Continued Pixel Refiner Epoch [9/30], Loss: 886276.2048\n",
      "Continued Pixel Refiner Epoch [10/30], Loss: 877052.1212\n",
      "Continued Pixel Refiner Epoch [11/30], Loss: 875393.2920\n",
      "Continued Pixel Refiner Epoch [12/30], Loss: 878483.9274\n",
      "Continued Pixel Refiner Epoch [13/30], Loss: 873131.6448\n",
      "Continued Pixel Refiner Epoch [14/30], Loss: 895449.7414\n",
      "Continued Pixel Refiner Epoch [15/30], Loss: 872132.4972\n",
      "Continued Pixel Refiner Epoch [16/30], Loss: 880617.4871\n",
      "Continued Pixel Refiner Epoch [17/30], Loss: 871563.1726\n",
      "Continued Pixel Refiner Epoch [18/30], Loss: 870306.8767\n",
      "Continued Pixel Refiner Epoch [19/30], Loss: 887044.7444\n",
      "Continued Pixel Refiner Epoch [20/30], Loss: 872079.1247\n",
      "Continued Pixel Refiner Epoch [21/30], Loss: 868416.4151\n",
      "Continued Pixel Refiner Epoch [22/30], Loss: 875002.3962\n",
      "Continued Pixel Refiner Epoch [23/30], Loss: 871159.2198\n",
      "Continued Pixel Refiner Epoch [24/30], Loss: 867257.2083\n",
      "Continued Pixel Refiner Epoch [25/30], Loss: 868221.4604\n",
      "Continued Pixel Refiner Epoch [26/30], Loss: 868256.0090\n",
      "Continued Pixel Refiner Epoch [27/30], Loss: 867638.2527\n",
      "Continued Pixel Refiner Epoch [28/30], Loss: 866916.0207\n",
      "Continued Pixel Refiner Epoch [29/30], Loss: 865643.7306\n",
      "Continued Pixel Refiner Epoch [30/30], Loss: 869872.5201\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T01:56:46.602926Z",
     "start_time": "2025-03-30T01:56:46.595651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.save(refiner_model.state_dict(), 'pixel_refiner_continued.pth')\n",
    "print(\"Refiner model saved after additional training epochs!\")"
   ],
   "id": "b1e60f3b9bd0eb5e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refiner model saved after additional training epochs!\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T01:56:50.956567Z",
     "start_time": "2025-03-30T01:56:50.951704Z"
    }
   },
   "cell_type": "code",
   "source": "refiner_model.load_state_dict(torch.load('pixel_refiner_continued.pth', map_location=device))\n",
   "id": "fedfb0b436b74d71",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T01:46:15.626064Z",
     "start_time": "2025-03-30T01:46:08.952269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results = []\n",
    "model.eval()\n",
    "refiner_model.eval()\n",
    "\n",
    "for file_name in test_files:\n",
    "    image_id = int(file_name.split('.')[0])\n",
    "    img_path = os.path.join(test_images_path, file_name)\n",
    "\n",
    "    image = cv2.imread(img_path)\n",
    "    img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    img_tensor = transform(img_rgb).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_class, pred_bbox = model(img_tensor)\n",
    "\n",
    "    pred_class = pred_class.item()\n",
    "    pred_bbox = pred_bbox.cpu().numpy().flatten().astype(int)\n",
    "\n",
    "    duck_present = 1 if pred_class > 0.5 else 0\n",
    "\n",
    "    if duck_present:\n",
    "        x1, y1, x2, y2 = pred_bbox\n",
    "        bbox = f\"{x1} {y1} {x2} {y2}\"\n",
    "\n",
    "        bbox_img = image[y1:y2, x1:x2]\n",
    "        if bbox_img.size == 0:\n",
    "            bbox_img = np.zeros((32,32,3), dtype=np.uint8)\n",
    "\n",
    "        bbox_img_tensor = transform_refiner(bbox_img).unsqueeze(0).to(device)\n",
    "\n",
    "        # initial prediction\n",
    "        gray_bbox_img = cv2.cvtColor(bbox_img, cv2.COLOR_BGR2GRAY)\n",
    "        _, thresh = cv2.threshold(gray_bbox_img, 10, 255, cv2.THRESH_BINARY_INV)\n",
    "        initial_pixel_count = cv2.countNonZero(thresh)\n",
    "        initial_pixel_count_tensor = torch.tensor([[initial_pixel_count]], dtype=torch.float32).to(device)\n",
    "\n",
    "        # refined prediction\n",
    "        with torch.no_grad():\n",
    "            refined_pixel_count = refiner_model(bbox_img_tensor, initial_pixel_count_tensor).item()\n",
    "\n",
    "        pixel_count = int(refined_pixel_count)\n",
    "\n",
    "    else:\n",
    "        bbox = \"0 0 0 0\"\n",
    "        pixel_count = 0\n",
    "\n",
    "    results.append([image_id, duck_present, pixel_count, bbox])\n",
    "\n",
    "submission_df = pd.DataFrame(results, columns=['DatapointID', 'DuckOrNoDuck', 'PixelCount', 'BoundingBox'])\n",
    "submission_df.to_csv('submission.csv', index=False)\n"
   ],
   "id": "54b8bd7ef4accb10",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T01:16:54.152398Z",
     "start_time": "2025-03-30T01:16:53.996005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# First model (classification + bbox)\n",
    "first_model = DuckDetector().to(device)\n",
    "first_model.load_state_dict(torch.load('duck_detector_epoch30.pth', map_location=device))\n",
    "\n",
    "# Second model (fixed refinement)\n",
    "second_model = PixelRefiner().to(device)\n",
    "second_model.load_state_dict(torch.load('pixel_refiner_continued.pth', map_location=device))\n",
    "second_model.eval()  # ensure refinement model remains fixed\n"
   ],
   "id": "562018e1d9c7323e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ingridcorobana/PyCharmMiscProject/.venv/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/ingridcorobana/PyCharmMiscProject/.venv/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PixelRefiner(\n",
       "  (conv_layers): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): AdaptiveAvgPool2d(output_size=(4, 4))\n",
       "  )\n",
       "  (fc_attention): Sequential(\n",
       "    (0): Linear(in_features=513, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=1, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T01:32:16.450048Z",
     "start_time": "2025-03-30T01:17:07.079071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optimizer = torch.optim.Adam(first_model.parameters(), lr=1e-4)\n",
    "criterion_class = nn.BCELoss()\n",
    "criterion_bbox = nn.MSELoss()\n",
    "\n",
    "first_model.train()\n",
    "additional_epochs = 10\n",
    "\n",
    "for epoch in range(additional_epochs):\n",
    "    total_loss = 0\n",
    "    for images, labels, bboxes in loader:\n",
    "        images, labels, bboxes = images.to(device), labels.to(device), bboxes.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs_class, outputs_bbox = first_model(images)\n",
    "\n",
    "        loss_class = criterion_class(outputs_class, labels)\n",
    "\n",
    "        bbox_mask = labels.squeeze() == 1\n",
    "        if bbox_mask.any():\n",
    "            loss_bbox = criterion_bbox(outputs_bbox[bbox_mask], bboxes[bbox_mask])\n",
    "        else:\n",
    "            loss_bbox = torch.tensor(0.0).to(device)\n",
    "\n",
    "        loss = loss_class + loss_bbox\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    print(f\"Continued Epoch [{epoch+1}/{additional_epochs}], Loss: {avg_loss:.4f}\")\n"
   ],
   "id": "156b41ce21adf2fc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continued Epoch [1/10], Loss: 1873.6875\n",
      "Continued Epoch [2/10], Loss: 1808.1096\n",
      "Continued Epoch [3/10], Loss: 1754.0883\n",
      "Continued Epoch [4/10], Loss: 1796.5417\n",
      "Continued Epoch [5/10], Loss: 1774.1952\n",
      "Continued Epoch [6/10], Loss: 1731.2275\n",
      "Continued Epoch [7/10], Loss: 1633.4038\n",
      "Continued Epoch [8/10], Loss: 1604.5541\n",
      "Continued Epoch [9/10], Loss: 1611.6569\n",
      "Continued Epoch [10/10], Loss: 1664.2493\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T01:48:18.382812Z",
     "start_time": "2025-03-30T01:48:18.313965Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(first_model.state_dict(), 'duck_detector_epoch40.pth')",
   "id": "adf8e3a211aeaa33",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T01:50:49.863572Z",
     "start_time": "2025-03-30T01:50:49.800742Z"
    }
   },
   "cell_type": "code",
   "source": [
    "first_model.load_state_dict(torch.load('duck_detector_epoch40.pth', map_location=device))\n",
    "first_model.eval()\n"
   ],
   "id": "3708e7fc816c25d6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DuckDetector(\n",
       "  (base_model): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Identity()\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=128, out_features=1, bias=True)\n",
       "    (4): Sigmoid()\n",
       "  )\n",
       "  (bbox_regressor): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=128, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T02:11:15.584146Z",
     "start_time": "2025-03-30T02:11:08.659250Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results = []\n",
    "first_model.eval()\n",
    "refiner_model.eval()\n",
    "\n",
    "for file_name in test_files:\n",
    "    image_id = int(file_name.split('.')[0])\n",
    "    img_path = os.path.join(test_images_path, file_name)\n",
    "\n",
    "    image = cv2.imread(img_path)\n",
    "    img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    img_tensor = transform(img_rgb).unsqueeze(0).to(device)\n",
    "\n",
    "    # FIXED THIS CLEARLY: Use the correct (trained) model here\n",
    "    with torch.no_grad():\n",
    "        pred_class, pred_bbox = first_model(img_tensor)\n",
    "\n",
    "    pred_class = pred_class.item()\n",
    "    pred_bbox = pred_bbox.cpu().numpy().flatten().astype(int)\n",
    "\n",
    "    duck_present = 1 if pred_class > 0.5 else 0\n",
    "\n",
    "    if duck_present:\n",
    "        x1, y1, x2, y2 = pred_bbox\n",
    "        bbox = f\"{x1} {y1} {x2} {y2}\"\n",
    "\n",
    "        bbox_img = image[y1:y2, x1:x2]\n",
    "        if bbox_img.size == 0:\n",
    "            bbox_img = np.zeros((32,32,3), dtype=np.uint8)\n",
    "\n",
    "        bbox_img_tensor = transform_refiner(bbox_img).unsqueeze(0).to(device)\n",
    "\n",
    "        # initial prediction\n",
    "        gray_bbox_img = cv2.cvtColor(bbox_img, cv2.COLOR_BGR2GRAY)\n",
    "        _, thresh = cv2.threshold(gray_bbox_img, 10, 255, cv2.THRESH_BINARY_INV)\n",
    "        initial_pixel_count = cv2.countNonZero(thresh)\n",
    "        initial_pixel_count_tensor = torch.tensor([[initial_pixel_count]], dtype=torch.float32).to(device)\n",
    "\n",
    "        # refined prediction\n",
    "        with torch.no_grad():\n",
    "            refined_pixel_count = refiner_model(bbox_img_tensor, initial_pixel_count_tensor).item()\n",
    "\n",
    "        pixel_count = int(refined_pixel_count)\n",
    "\n",
    "    else:\n",
    "        bbox = \"0 0 0 0\"\n",
    "        pixel_count = 0\n",
    "\n",
    "    results.append([image_id, duck_present, pixel_count, bbox])\n",
    "\n",
    "submission_df = pd.DataFrame(results, columns=['DatapointID', 'DuckOrNoDuck', 'PixelCount', 'BoundingBox'])\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n"
   ],
   "id": "869aba41a3198bdc",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T02:08:03.761436Z",
     "start_time": "2025-03-30T02:04:52.920912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "refiner_model.load_state_dict(torch.load('pixel_refiner_continued.pth', map_location=device))\n",
    "\n",
    "additional_epochs = 30  # Add how many more epochs you want\n",
    "\n",
    "refiner_model.train()\n",
    "\n",
    "for epoch in range(additional_epochs):\n",
    "    total_loss = 0\n",
    "    for imgs, initial_preds, gt_pixels in refiner_loader:\n",
    "        imgs, initial_preds, gt_pixels = imgs.to(device), initial_preds.to(device), gt_pixels.to(device)\n",
    "\n",
    "        optimizer_refiner.zero_grad()\n",
    "        refined_output = refiner_model(imgs, initial_preds)\n",
    "        loss = criterion_refiner(refined_output, gt_pixels)\n",
    "        loss.backward()\n",
    "        optimizer_refiner.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(refiner_loader)\n",
    "    print(f\"Continued Pixel Refiner Epoch [{epoch + 1}/{additional_epochs}], Loss: {avg_loss:.4f}\")\n",
    "torch.save(refiner_model.state_dict(), 'pixel_refiner_continued.pth')\n",
    "print(\"Refiner model saved after additional training epochs!\")"
   ],
   "id": "6d1d89e6396eca67",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continued Pixel Refiner Epoch [1/30], Loss: 867247.4588\n",
      "Continued Pixel Refiner Epoch [2/30], Loss: 880247.6973\n",
      "Continued Pixel Refiner Epoch [3/30], Loss: 866849.7111\n",
      "Continued Pixel Refiner Epoch [4/30], Loss: 864974.8598\n",
      "Continued Pixel Refiner Epoch [5/30], Loss: 867865.4273\n",
      "Continued Pixel Refiner Epoch [6/30], Loss: 872305.6505\n",
      "Continued Pixel Refiner Epoch [7/30], Loss: 864377.0539\n",
      "Continued Pixel Refiner Epoch [8/30], Loss: 887525.7345\n",
      "Continued Pixel Refiner Epoch [9/30], Loss: 874765.7110\n",
      "Continued Pixel Refiner Epoch [10/30], Loss: 873060.5463\n",
      "Continued Pixel Refiner Epoch [11/30], Loss: 868677.5329\n",
      "Continued Pixel Refiner Epoch [12/30], Loss: 864041.8428\n",
      "Continued Pixel Refiner Epoch [13/30], Loss: 864085.9435\n",
      "Continued Pixel Refiner Epoch [14/30], Loss: 863256.4262\n",
      "Continued Pixel Refiner Epoch [15/30], Loss: 866231.1448\n",
      "Continued Pixel Refiner Epoch [16/30], Loss: 867151.7416\n",
      "Continued Pixel Refiner Epoch [17/30], Loss: 862925.5605\n",
      "Continued Pixel Refiner Epoch [18/30], Loss: 873318.0955\n",
      "Continued Pixel Refiner Epoch [19/30], Loss: 869961.3800\n",
      "Continued Pixel Refiner Epoch [20/30], Loss: 863315.9302\n",
      "Continued Pixel Refiner Epoch [21/30], Loss: 862500.5375\n",
      "Continued Pixel Refiner Epoch [22/30], Loss: 868511.8728\n",
      "Continued Pixel Refiner Epoch [23/30], Loss: 861843.7581\n",
      "Continued Pixel Refiner Epoch [24/30], Loss: 872873.4658\n",
      "Continued Pixel Refiner Epoch [25/30], Loss: 863081.7368\n",
      "Continued Pixel Refiner Epoch [26/30], Loss: 860741.7747\n",
      "Continued Pixel Refiner Epoch [27/30], Loss: 878981.5751\n",
      "Continued Pixel Refiner Epoch [28/30], Loss: 860247.9814\n",
      "Continued Pixel Refiner Epoch [29/30], Loss: 866180.9582\n",
      "Continued Pixel Refiner Epoch [30/30], Loss: 862139.3560\n",
      "Refiner model saved after additional training epochs!\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T02:10:58.035596Z",
     "start_time": "2025-03-30T02:10:58.028386Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Continued Pixel Refiner Epoch [{epoch + 1}/{additional_epochs}], Loss: {avg_loss:.4f}\")\n",
    "torch.save(refiner_model.state_dict(), 'pixel_refiner_continued.pth')\n",
    "print(\"Refiner model saved after additional training epochs!\")"
   ],
   "id": "5514865499529f61",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continued Pixel Refiner Epoch [30/30], Loss: 862139.3560\n",
      "Refiner model saved after additional training epochs!\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T02:13:11.677873Z",
     "start_time": "2025-03-30T02:13:11.518915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# clearly define and load your freshly trained model\n",
    "first_model = DuckDetector().to(device)\n",
    "first_model.load_state_dict(torch.load('duck_detector_epoch40.pth', map_location=device))\n",
    "first_model.eval()\n",
    "\n",
    "# Also ensure refiner model is correctly loaded\n",
    "refiner_model.load_state_dict(torch.load('pixel_refiner_continued.pth', map_location=device))\n",
    "refiner_model.eval()\n"
   ],
   "id": "e2d683cf78996319",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ingridcorobana/PyCharmMiscProject/.venv/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/ingridcorobana/PyCharmMiscProject/.venv/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PixelRefiner(\n",
       "  (conv_layers): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): AdaptiveAvgPool2d(output_size=(4, 4))\n",
       "  )\n",
       "  (fc_attention): Sequential(\n",
       "    (0): Linear(in_features=513, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=1, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T02:14:20.590322Z",
     "start_time": "2025-03-30T02:14:13.449819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results = []\n",
    "\n",
    "for file_name in test_files:\n",
    "    image_id = int(file_name.split('.')[0])\n",
    "    img_path = os.path.join(test_images_path, file_name)\n",
    "\n",
    "    image = cv2.imread(img_path)\n",
    "    img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    img_tensor = transform(img_rgb).unsqueeze(0).to(device)\n",
    "\n",
    "    # clearly use loaded first_model here\n",
    "    with torch.no_grad():\n",
    "        pred_class, pred_bbox = first_model(img_tensor)\n",
    "\n",
    "    pred_class = pred_class.item()\n",
    "    pred_bbox = pred_bbox.cpu().numpy().flatten().astype(int)\n",
    "\n",
    "    duck_present = 1 if pred_class > 0.5 else 0\n",
    "\n",
    "    if duck_present:\n",
    "        x1, y1, x2, y2 = pred_bbox\n",
    "        bbox = f\"{x1} {y1} {x2} {y2}\"\n",
    "\n",
    "        bbox_img = image[y1:y2, x1:x2]\n",
    "        if bbox_img.size == 0:\n",
    "            bbox_img = np.zeros((32,32,3), dtype=np.uint8)\n",
    "\n",
    "        bbox_img_tensor = transform_refiner(bbox_img).unsqueeze(0).to(device)\n",
    "\n",
    "        # initial prediction\n",
    "        gray_bbox_img = cv2.cvtColor(bbox_img, cv2.COLOR_BGR2GRAY)\n",
    "        _, thresh = cv2.threshold(gray_bbox_img, 10, 255, cv2.THRESH_BINARY_INV)\n",
    "        initial_pixel_count = cv2.countNonZero(thresh)\n",
    "        initial_pixel_count_tensor = torch.tensor([[initial_pixel_count]], dtype=torch.float32).to(device)\n",
    "\n",
    "        # refined prediction\n",
    "        with torch.no_grad():\n",
    "            refined_pixel_count = refiner_model(bbox_img_tensor, initial_pixel_count_tensor).item()\n",
    "\n",
    "        pixel_count = int(refined_pixel_count)\n",
    "\n",
    "    else:\n",
    "        bbox = \"0 0 0 0\"\n",
    "        pixel_count = 0\n",
    "\n",
    "    results.append([image_id, duck_present, pixel_count, bbox])\n",
    "\n",
    "submission_df = pd.DataFrame(results, columns=['DatapointID', 'DuckOrNoDuck', 'PixelCount', 'BoundingBox'])\n",
    "submission_df.to_csv('submission.csv', index=False)\n"
   ],
   "id": "f3c52b32d13ebd44",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T02:32:44.868392Z",
     "start_time": "2025-03-30T02:17:04.567660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optimizer = torch.optim.Adam(first_model.parameters(), lr=1e-4)\n",
    "criterion_class = nn.BCELoss()\n",
    "criterion_bbox = nn.MSELoss()\n",
    "\n",
    "first_model.train()\n",
    "additional_epochs = 10\n",
    "\n",
    "for epoch in range(additional_epochs):\n",
    "    total_loss = 0\n",
    "    for images, labels, bboxes in loader:\n",
    "        images, labels, bboxes = images.to(device), labels.to(device), bboxes.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs_class, outputs_bbox = first_model(images)\n",
    "\n",
    "        loss_class = criterion_class(outputs_class, labels)\n",
    "\n",
    "        bbox_mask = labels.squeeze() == 1\n",
    "        if bbox_mask.any():\n",
    "            loss_bbox = criterion_bbox(outputs_bbox[bbox_mask], bboxes[bbox_mask])\n",
    "        else:\n",
    "            loss_bbox = torch.tensor(0.0).to(device)\n",
    "\n",
    "        loss = loss_class + loss_bbox\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    print(f\"Continued Epoch [{epoch+1}/{additional_epochs}], Loss: {avg_loss:.4f}\")\n"
   ],
   "id": "c2d34ffcf3a8a6fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continued Epoch [1/10], Loss: 1570.5534\n",
      "Continued Epoch [2/10], Loss: 1524.5895\n",
      "Continued Epoch [3/10], Loss: 1516.5384\n",
      "Continued Epoch [4/10], Loss: 1511.1019\n",
      "Continued Epoch [5/10], Loss: 1381.4297\n",
      "Continued Epoch [6/10], Loss: 1363.5008\n",
      "Continued Epoch [7/10], Loss: 1386.3165\n",
      "Continued Epoch [8/10], Loss: 1304.4864\n",
      "Continued Epoch [9/10], Loss: 1294.9442\n",
      "Continued Epoch [10/10], Loss: 1266.0421\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T02:38:20.586227Z",
     "start_time": "2025-03-30T02:38:20.489038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.save(first_model.state_dict(), 'duck_detector_epoch40.pth')\n",
    "first_model.load_state_dict(torch.load('duck_detector_epoch40.pth', map_location=device))\n",
    "first_model.eval()\n"
   ],
   "id": "fdb67908c56a2694",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DuckDetector(\n",
       "  (base_model): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Identity()\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=128, out_features=1, bias=True)\n",
       "    (4): Sigmoid()\n",
       "  )\n",
       "  (bbox_regressor): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=128, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T02:38:54.021646Z",
     "start_time": "2025-03-30T02:38:47.287116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results = []\n",
    "\n",
    "for file_name in test_files:\n",
    "    image_id = int(file_name.split('.')[0])\n",
    "    img_path = os.path.join(test_images_path, file_name)\n",
    "\n",
    "    image = cv2.imread(img_path)\n",
    "    img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    img_tensor = transform(img_rgb).unsqueeze(0).to(device)\n",
    "\n",
    "    # clearly use loaded first_model here\n",
    "    with torch.no_grad():\n",
    "        pred_class, pred_bbox = first_model(img_tensor)\n",
    "\n",
    "    pred_class = pred_class.item()\n",
    "    pred_bbox = pred_bbox.cpu().numpy().flatten().astype(int)\n",
    "\n",
    "    duck_present = 1 if pred_class > 0.5 else 0\n",
    "\n",
    "    if duck_present:\n",
    "        x1, y1, x2, y2 = pred_bbox\n",
    "        bbox = f\"{x1} {y1} {x2} {y2}\"\n",
    "\n",
    "        bbox_img = image[y1:y2, x1:x2]\n",
    "        if bbox_img.size == 0:\n",
    "            bbox_img = np.zeros((32,32,3), dtype=np.uint8)\n",
    "\n",
    "        bbox_img_tensor = transform_refiner(bbox_img).unsqueeze(0).to(device)\n",
    "\n",
    "        # initial prediction\n",
    "        gray_bbox_img = cv2.cvtColor(bbox_img, cv2.COLOR_BGR2GRAY)\n",
    "        _, thresh = cv2.threshold(gray_bbox_img, 10, 255, cv2.THRESH_BINARY_INV)\n",
    "        initial_pixel_count = cv2.countNonZero(thresh)\n",
    "        initial_pixel_count_tensor = torch.tensor([[initial_pixel_count]], dtype=torch.float32).to(device)\n",
    "\n",
    "        # refined prediction\n",
    "        with torch.no_grad():\n",
    "            refined_pixel_count = refiner_model(bbox_img_tensor, initial_pixel_count_tensor).item()\n",
    "\n",
    "        pixel_count = int(refined_pixel_count)\n",
    "\n",
    "    else:\n",
    "        bbox = \"0 0 0 0\"\n",
    "        pixel_count = 0\n",
    "\n",
    "    results.append([image_id, duck_present, pixel_count, bbox])\n",
    "\n",
    "submission_df = pd.DataFrame(results, columns=['DatapointID', 'DuckOrNoDuck', 'PixelCount', 'BoundingBox'])\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ],
   "id": "57d56ee7bc11a038",
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T02:47:44.741218Z",
     "start_time": "2025-03-30T02:46:51.894559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "first_model.eval()\n",
    "updated_refiner_data = []\n",
    "\n",
    "for images, labels, gt_bboxes in loader:\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_class, pred_bbox = first_model(images)\n",
    "\n",
    "    pred_bbox = pred_bbox.cpu().numpy().astype(int)\n",
    "    images_np = images.cpu().numpy()\n",
    "\n",
    "    for idx in range(len(images)):\n",
    "        if labels[idx].item() == 1:\n",
    "            x1, y1, x2, y2 = pred_bbox[idx]\n",
    "            img = images_np[idx].transpose(1,2,0) * 255\n",
    "            img = img.astype(np.uint8)\n",
    "\n",
    "            bbox_img = img[y1:y2, x1:x2]\n",
    "            if bbox_img.size == 0:\n",
    "                bbox_img = np.zeros((32,32,3), dtype=np.uint8)\n",
    "\n",
    "            # Ground truth pixel count\n",
    "            gt_bbox = gt_bboxes[idx].cpu().numpy().astype(int)\n",
    "            gt_x1, gt_y1, gt_x2, gt_y2 = gt_bbox\n",
    "            gt_pixel_count = (gt_x2 - gt_x1) * (gt_y2 - gt_y1)\n",
    "\n",
    "            # Initial prediction using thresholding\n",
    "            gray_bbox_img = cv2.cvtColor(bbox_img, cv2.COLOR_BGR2GRAY)\n",
    "            _, thresh = cv2.threshold(gray_bbox_img, 10, 255, cv2.THRESH_BINARY_INV)\n",
    "            initial_pixel_count = cv2.countNonZero(thresh)\n",
    "\n",
    "            updated_refiner_data.append((bbox_img, initial_pixel_count, gt_pixel_count))\n"
   ],
   "id": "1975dadf0e057e26",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T02:48:32.059149Z",
     "start_time": "2025-03-30T02:48:32.053100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class UpdatedRefinerDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, initial_pred, gt_pixel_count = self.data[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, torch.tensor([initial_pred], dtype=torch.float32), torch.tensor([gt_pixel_count], dtype=torch.float32)\n",
    "\n",
    "transform_refiner = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((64,64)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "updated_dataset = UpdatedRefinerDataset(updated_refiner_data, transform=transform_refiner)\n",
    "updated_loader = DataLoader(updated_dataset, batch_size=16, shuffle=True)\n"
   ],
   "id": "10aa3bfd11837fef",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T03:00:20.351744Z",
     "start_time": "2025-03-30T02:50:23.157393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Clearly reload previously trained Pixel Refiner\n",
    "refiner_model.load_state_dict(torch.load('pixel_refiner_continued.pth', map_location=device))\n",
    "refiner_model.train()\n",
    "\n",
    "criterion_refiner = nn.MSELoss()\n",
    "optimizer_refiner = torch.optim.Adam(refiner_model.parameters(), lr=1e-5)  # use lower LR for fine-tuning\n",
    "\n",
    "fine_tune_epochs = 500\n",
    "for epoch in range(fine_tune_epochs):\n",
    "    total_loss = 0\n",
    "    for imgs, initial_preds, gt_pixels in updated_loader:\n",
    "        imgs, initial_preds, gt_pixels = imgs.to(device), initial_preds.to(device), gt_pixels.to(device)\n",
    "\n",
    "        optimizer_refiner.zero_grad()\n",
    "        refined_output = refiner_model(imgs, initial_preds)\n",
    "        loss = criterion_refiner(refined_output, gt_pixels)\n",
    "        loss.backward()\n",
    "        optimizer_refiner.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(updated_loader)\n",
    "    print(f\"Fine-tune Pixel Refiner Epoch [{epoch+1}/{fine_tune_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Save fine-tuned Pixel Refiner clearly\n",
    "torch.save(refiner_model.state_dict(), 'pixel_refiner_finetuned.pth')\n",
    "print(\"Pixel Refiner fine-tuned and saved successfully!\")\n"
   ],
   "id": "23f6d2129e801a6c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Pixel Refiner Epoch [1/500], Loss: 178285390.2796\n",
      "Fine-tune Pixel Refiner Epoch [2/500], Loss: 177023110.8387\n",
      "Fine-tune Pixel Refiner Epoch [3/500], Loss: 175386419.4839\n",
      "Fine-tune Pixel Refiner Epoch [4/500], Loss: 174279361.2043\n",
      "Fine-tune Pixel Refiner Epoch [5/500], Loss: 172622228.8172\n",
      "Fine-tune Pixel Refiner Epoch [6/500], Loss: 171037992.9032\n",
      "Fine-tune Pixel Refiner Epoch [7/500], Loss: 169389656.0000\n",
      "Fine-tune Pixel Refiner Epoch [8/500], Loss: 167824576.3441\n",
      "Fine-tune Pixel Refiner Epoch [9/500], Loss: 166127732.3011\n",
      "Fine-tune Pixel Refiner Epoch [10/500], Loss: 164909853.0753\n",
      "Fine-tune Pixel Refiner Epoch [11/500], Loss: 162903632.2581\n",
      "Fine-tune Pixel Refiner Epoch [12/500], Loss: 161154578.1935\n",
      "Fine-tune Pixel Refiner Epoch [13/500], Loss: 159504335.0968\n",
      "Fine-tune Pixel Refiner Epoch [14/500], Loss: 158024813.2473\n",
      "Fine-tune Pixel Refiner Epoch [15/500], Loss: 156422699.7849\n",
      "Fine-tune Pixel Refiner Epoch [16/500], Loss: 154515551.2688\n",
      "Fine-tune Pixel Refiner Epoch [17/500], Loss: 152902965.2903\n",
      "Fine-tune Pixel Refiner Epoch [18/500], Loss: 151257419.9140\n",
      "Fine-tune Pixel Refiner Epoch [19/500], Loss: 149548360.5161\n",
      "Fine-tune Pixel Refiner Epoch [20/500], Loss: 147812508.1720\n",
      "Fine-tune Pixel Refiner Epoch [21/500], Loss: 146384844.4301\n",
      "Fine-tune Pixel Refiner Epoch [22/500], Loss: 144531614.7312\n",
      "Fine-tune Pixel Refiner Epoch [23/500], Loss: 142991703.8710\n",
      "Fine-tune Pixel Refiner Epoch [24/500], Loss: 141384884.6452\n",
      "Fine-tune Pixel Refiner Epoch [25/500], Loss: 139869280.5591\n",
      "Fine-tune Pixel Refiner Epoch [26/500], Loss: 138599220.3441\n",
      "Fine-tune Pixel Refiner Epoch [27/500], Loss: 137009183.1183\n",
      "Fine-tune Pixel Refiner Epoch [28/500], Loss: 135593633.0753\n",
      "Fine-tune Pixel Refiner Epoch [29/500], Loss: 134341618.0860\n",
      "Fine-tune Pixel Refiner Epoch [30/500], Loss: 133037746.1828\n",
      "Fine-tune Pixel Refiner Epoch [31/500], Loss: 131841152.7097\n",
      "Fine-tune Pixel Refiner Epoch [32/500], Loss: 130540763.0323\n",
      "Fine-tune Pixel Refiner Epoch [33/500], Loss: 129533068.6667\n",
      "Fine-tune Pixel Refiner Epoch [34/500], Loss: 128537647.5699\n",
      "Fine-tune Pixel Refiner Epoch [35/500], Loss: 127474786.6452\n",
      "Fine-tune Pixel Refiner Epoch [36/500], Loss: 126542414.1720\n",
      "Fine-tune Pixel Refiner Epoch [37/500], Loss: 125596382.0645\n",
      "Fine-tune Pixel Refiner Epoch [38/500], Loss: 125078974.2796\n",
      "Fine-tune Pixel Refiner Epoch [39/500], Loss: 124232173.7634\n",
      "Fine-tune Pixel Refiner Epoch [40/500], Loss: 123725231.6559\n",
      "Fine-tune Pixel Refiner Epoch [41/500], Loss: 123247136.6022\n",
      "Fine-tune Pixel Refiner Epoch [42/500], Loss: 122580175.1398\n",
      "Fine-tune Pixel Refiner Epoch [43/500], Loss: 122065072.4731\n",
      "Fine-tune Pixel Refiner Epoch [44/500], Loss: 121526416.4946\n",
      "Fine-tune Pixel Refiner Epoch [45/500], Loss: 121292532.2796\n",
      "Fine-tune Pixel Refiner Epoch [46/500], Loss: 121067324.7312\n",
      "Fine-tune Pixel Refiner Epoch [47/500], Loss: 120585037.8710\n",
      "Fine-tune Pixel Refiner Epoch [48/500], Loss: 120277226.6237\n",
      "Fine-tune Pixel Refiner Epoch [49/500], Loss: 120002712.1720\n",
      "Fine-tune Pixel Refiner Epoch [50/500], Loss: 119705660.6022\n",
      "Fine-tune Pixel Refiner Epoch [51/500], Loss: 119395733.1183\n",
      "Fine-tune Pixel Refiner Epoch [52/500], Loss: 119347652.5161\n",
      "Fine-tune Pixel Refiner Epoch [53/500], Loss: 119001803.6989\n",
      "Fine-tune Pixel Refiner Epoch [54/500], Loss: 119048976.0000\n",
      "Fine-tune Pixel Refiner Epoch [55/500], Loss: 118580329.4194\n",
      "Fine-tune Pixel Refiner Epoch [56/500], Loss: 118475636.4301\n",
      "Fine-tune Pixel Refiner Epoch [57/500], Loss: 118320447.9570\n",
      "Fine-tune Pixel Refiner Epoch [58/500], Loss: 118200532.9247\n",
      "Fine-tune Pixel Refiner Epoch [59/500], Loss: 117985221.0323\n",
      "Fine-tune Pixel Refiner Epoch [60/500], Loss: 117908204.3656\n",
      "Fine-tune Pixel Refiner Epoch [61/500], Loss: 117812420.3226\n",
      "Fine-tune Pixel Refiner Epoch [62/500], Loss: 117526744.3871\n",
      "Fine-tune Pixel Refiner Epoch [63/500], Loss: 117343512.9032\n",
      "Fine-tune Pixel Refiner Epoch [64/500], Loss: 117358597.2043\n",
      "Fine-tune Pixel Refiner Epoch [65/500], Loss: 117147695.6989\n",
      "Fine-tune Pixel Refiner Epoch [66/500], Loss: 116947341.6344\n",
      "Fine-tune Pixel Refiner Epoch [67/500], Loss: 116963191.6129\n",
      "Fine-tune Pixel Refiner Epoch [68/500], Loss: 116682035.3118\n",
      "Fine-tune Pixel Refiner Epoch [69/500], Loss: 116696376.3011\n",
      "Fine-tune Pixel Refiner Epoch [70/500], Loss: 116496691.3333\n",
      "Fine-tune Pixel Refiner Epoch [71/500], Loss: 116339064.5376\n",
      "Fine-tune Pixel Refiner Epoch [72/500], Loss: 116268422.3656\n",
      "Fine-tune Pixel Refiner Epoch [73/500], Loss: 115985382.3656\n",
      "Fine-tune Pixel Refiner Epoch [74/500], Loss: 115861348.2796\n",
      "Fine-tune Pixel Refiner Epoch [75/500], Loss: 115782301.2903\n",
      "Fine-tune Pixel Refiner Epoch [76/500], Loss: 115656842.3871\n",
      "Fine-tune Pixel Refiner Epoch [77/500], Loss: 115502799.4839\n",
      "Fine-tune Pixel Refiner Epoch [78/500], Loss: 115337764.1290\n",
      "Fine-tune Pixel Refiner Epoch [79/500], Loss: 115140265.7419\n",
      "Fine-tune Pixel Refiner Epoch [80/500], Loss: 115161850.4086\n",
      "Fine-tune Pixel Refiner Epoch [81/500], Loss: 114929504.9677\n",
      "Fine-tune Pixel Refiner Epoch [82/500], Loss: 114825884.8602\n",
      "Fine-tune Pixel Refiner Epoch [83/500], Loss: 114674776.5806\n",
      "Fine-tune Pixel Refiner Epoch [84/500], Loss: 114510136.7312\n",
      "Fine-tune Pixel Refiner Epoch [85/500], Loss: 114410021.0753\n",
      "Fine-tune Pixel Refiner Epoch [86/500], Loss: 114193221.7204\n",
      "Fine-tune Pixel Refiner Epoch [87/500], Loss: 114059721.8065\n",
      "Fine-tune Pixel Refiner Epoch [88/500], Loss: 114018816.9247\n",
      "Fine-tune Pixel Refiner Epoch [89/500], Loss: 113772561.1613\n",
      "Fine-tune Pixel Refiner Epoch [90/500], Loss: 113798096.9247\n",
      "Fine-tune Pixel Refiner Epoch [91/500], Loss: 113597587.4409\n",
      "Fine-tune Pixel Refiner Epoch [92/500], Loss: 113574143.8925\n",
      "Fine-tune Pixel Refiner Epoch [93/500], Loss: 113291067.7849\n",
      "Fine-tune Pixel Refiner Epoch [94/500], Loss: 113155712.5376\n",
      "Fine-tune Pixel Refiner Epoch [95/500], Loss: 113060514.8387\n",
      "Fine-tune Pixel Refiner Epoch [96/500], Loss: 112844561.1398\n",
      "Fine-tune Pixel Refiner Epoch [97/500], Loss: 112743990.7527\n",
      "Fine-tune Pixel Refiner Epoch [98/500], Loss: 112693797.9140\n",
      "Fine-tune Pixel Refiner Epoch [99/500], Loss: 112519431.1505\n",
      "Fine-tune Pixel Refiner Epoch [100/500], Loss: 112375123.5699\n",
      "Fine-tune Pixel Refiner Epoch [101/500], Loss: 112168618.2796\n",
      "Fine-tune Pixel Refiner Epoch [102/500], Loss: 112057873.1183\n",
      "Fine-tune Pixel Refiner Epoch [103/500], Loss: 111871179.8710\n",
      "Fine-tune Pixel Refiner Epoch [104/500], Loss: 111939200.7312\n",
      "Fine-tune Pixel Refiner Epoch [105/500], Loss: 111479724.6237\n",
      "Fine-tune Pixel Refiner Epoch [106/500], Loss: 111564189.1183\n",
      "Fine-tune Pixel Refiner Epoch [107/500], Loss: 111411944.1075\n",
      "Fine-tune Pixel Refiner Epoch [108/500], Loss: 111162798.9032\n",
      "Fine-tune Pixel Refiner Epoch [109/500], Loss: 110988389.2903\n",
      "Fine-tune Pixel Refiner Epoch [110/500], Loss: 110873297.8495\n",
      "Fine-tune Pixel Refiner Epoch [111/500], Loss: 110819586.0215\n",
      "Fine-tune Pixel Refiner Epoch [112/500], Loss: 110560761.1398\n",
      "Fine-tune Pixel Refiner Epoch [113/500], Loss: 110493176.0000\n",
      "Fine-tune Pixel Refiner Epoch [114/500], Loss: 110435228.1505\n",
      "Fine-tune Pixel Refiner Epoch [115/500], Loss: 110065641.0108\n",
      "Fine-tune Pixel Refiner Epoch [116/500], Loss: 110098010.8817\n",
      "Fine-tune Pixel Refiner Epoch [117/500], Loss: 109771644.5161\n",
      "Fine-tune Pixel Refiner Epoch [118/500], Loss: 109936881.2043\n",
      "Fine-tune Pixel Refiner Epoch [119/500], Loss: 109584330.7742\n",
      "Fine-tune Pixel Refiner Epoch [120/500], Loss: 109562065.2258\n",
      "Fine-tune Pixel Refiner Epoch [121/500], Loss: 109233284.3871\n",
      "Fine-tune Pixel Refiner Epoch [122/500], Loss: 109296077.3333\n",
      "Fine-tune Pixel Refiner Epoch [123/500], Loss: 108895212.9892\n",
      "Fine-tune Pixel Refiner Epoch [124/500], Loss: 108771734.4731\n",
      "Fine-tune Pixel Refiner Epoch [125/500], Loss: 108644008.6452\n",
      "Fine-tune Pixel Refiner Epoch [126/500], Loss: 108430951.3978\n",
      "Fine-tune Pixel Refiner Epoch [127/500], Loss: 108440776.6882\n",
      "Fine-tune Pixel Refiner Epoch [128/500], Loss: 108258224.6237\n",
      "Fine-tune Pixel Refiner Epoch [129/500], Loss: 107996885.1613\n",
      "Fine-tune Pixel Refiner Epoch [130/500], Loss: 107864343.0968\n",
      "Fine-tune Pixel Refiner Epoch [131/500], Loss: 107749302.0430\n",
      "Fine-tune Pixel Refiner Epoch [132/500], Loss: 107584669.2903\n",
      "Fine-tune Pixel Refiner Epoch [133/500], Loss: 107478273.0538\n",
      "Fine-tune Pixel Refiner Epoch [134/500], Loss: 107276488.7312\n",
      "Fine-tune Pixel Refiner Epoch [135/500], Loss: 107102845.5699\n",
      "Fine-tune Pixel Refiner Epoch [136/500], Loss: 107000823.5269\n",
      "Fine-tune Pixel Refiner Epoch [137/500], Loss: 107076521.5914\n",
      "Fine-tune Pixel Refiner Epoch [138/500], Loss: 106831893.4409\n",
      "Fine-tune Pixel Refiner Epoch [139/500], Loss: 106554221.5269\n",
      "Fine-tune Pixel Refiner Epoch [140/500], Loss: 106526951.4624\n",
      "Fine-tune Pixel Refiner Epoch [141/500], Loss: 106179231.0108\n",
      "Fine-tune Pixel Refiner Epoch [142/500], Loss: 106119690.6882\n",
      "Fine-tune Pixel Refiner Epoch [143/500], Loss: 105911119.1828\n",
      "Fine-tune Pixel Refiner Epoch [144/500], Loss: 105865816.4086\n",
      "Fine-tune Pixel Refiner Epoch [145/500], Loss: 105603316.8172\n",
      "Fine-tune Pixel Refiner Epoch [146/500], Loss: 105429659.6344\n",
      "Fine-tune Pixel Refiner Epoch [147/500], Loss: 105312981.0108\n",
      "Fine-tune Pixel Refiner Epoch [148/500], Loss: 105118303.3118\n",
      "Fine-tune Pixel Refiner Epoch [149/500], Loss: 105006015.2151\n",
      "Fine-tune Pixel Refiner Epoch [150/500], Loss: 104840227.0323\n",
      "Fine-tune Pixel Refiner Epoch [151/500], Loss: 104656131.6344\n",
      "Fine-tune Pixel Refiner Epoch [152/500], Loss: 104544573.8280\n",
      "Fine-tune Pixel Refiner Epoch [153/500], Loss: 104426144.8602\n",
      "Fine-tune Pixel Refiner Epoch [154/500], Loss: 104201944.4946\n",
      "Fine-tune Pixel Refiner Epoch [155/500], Loss: 104080296.1720\n",
      "Fine-tune Pixel Refiner Epoch [156/500], Loss: 103905125.0753\n",
      "Fine-tune Pixel Refiner Epoch [157/500], Loss: 103793027.4624\n",
      "Fine-tune Pixel Refiner Epoch [158/500], Loss: 103664259.1828\n",
      "Fine-tune Pixel Refiner Epoch [159/500], Loss: 103594979.2473\n",
      "Fine-tune Pixel Refiner Epoch [160/500], Loss: 103312599.0323\n",
      "Fine-tune Pixel Refiner Epoch [161/500], Loss: 103152317.2473\n",
      "Fine-tune Pixel Refiner Epoch [162/500], Loss: 103047088.0215\n",
      "Fine-tune Pixel Refiner Epoch [163/500], Loss: 102850216.7312\n",
      "Fine-tune Pixel Refiner Epoch [164/500], Loss: 102679188.4086\n",
      "Fine-tune Pixel Refiner Epoch [165/500], Loss: 102720457.7204\n",
      "Fine-tune Pixel Refiner Epoch [166/500], Loss: 102416676.1075\n",
      "Fine-tune Pixel Refiner Epoch [167/500], Loss: 102276518.5161\n",
      "Fine-tune Pixel Refiner Epoch [168/500], Loss: 102132982.6237\n",
      "Fine-tune Pixel Refiner Epoch [169/500], Loss: 101900773.3118\n",
      "Fine-tune Pixel Refiner Epoch [170/500], Loss: 101810635.8065\n",
      "Fine-tune Pixel Refiner Epoch [171/500], Loss: 101634571.0968\n",
      "Fine-tune Pixel Refiner Epoch [172/500], Loss: 101477740.6452\n",
      "Fine-tune Pixel Refiner Epoch [173/500], Loss: 101252989.9355\n",
      "Fine-tune Pixel Refiner Epoch [174/500], Loss: 101109481.3978\n",
      "Fine-tune Pixel Refiner Epoch [175/500], Loss: 101030881.5484\n",
      "Fine-tune Pixel Refiner Epoch [176/500], Loss: 100873420.7097\n",
      "Fine-tune Pixel Refiner Epoch [177/500], Loss: 100624105.7634\n",
      "Fine-tune Pixel Refiner Epoch [178/500], Loss: 100447457.3118\n",
      "Fine-tune Pixel Refiner Epoch [179/500], Loss: 100381754.8172\n",
      "Fine-tune Pixel Refiner Epoch [180/500], Loss: 100099677.1183\n",
      "Fine-tune Pixel Refiner Epoch [181/500], Loss: 100083514.6237\n",
      "Fine-tune Pixel Refiner Epoch [182/500], Loss: 100129604.2796\n",
      "Fine-tune Pixel Refiner Epoch [183/500], Loss: 99695502.7097\n",
      "Fine-tune Pixel Refiner Epoch [184/500], Loss: 99586813.1183\n",
      "Fine-tune Pixel Refiner Epoch [185/500], Loss: 99502972.1075\n",
      "Fine-tune Pixel Refiner Epoch [186/500], Loss: 99214520.9462\n",
      "Fine-tune Pixel Refiner Epoch [187/500], Loss: 99306999.2473\n",
      "Fine-tune Pixel Refiner Epoch [188/500], Loss: 98871672.8817\n",
      "Fine-tune Pixel Refiner Epoch [189/500], Loss: 98758176.1075\n",
      "Fine-tune Pixel Refiner Epoch [190/500], Loss: 98553969.8065\n",
      "Fine-tune Pixel Refiner Epoch [191/500], Loss: 98683306.0430\n",
      "Fine-tune Pixel Refiner Epoch [192/500], Loss: 98320075.9570\n",
      "Fine-tune Pixel Refiner Epoch [193/500], Loss: 98072884.0645\n",
      "Fine-tune Pixel Refiner Epoch [194/500], Loss: 97922820.1613\n",
      "Fine-tune Pixel Refiner Epoch [195/500], Loss: 97769072.9677\n",
      "Fine-tune Pixel Refiner Epoch [196/500], Loss: 97583332.9032\n",
      "Fine-tune Pixel Refiner Epoch [197/500], Loss: 97436993.9140\n",
      "Fine-tune Pixel Refiner Epoch [198/500], Loss: 97317341.3548\n",
      "Fine-tune Pixel Refiner Epoch [199/500], Loss: 97145056.3011\n",
      "Fine-tune Pixel Refiner Epoch [200/500], Loss: 96939228.3441\n",
      "Fine-tune Pixel Refiner Epoch [201/500], Loss: 96928549.0108\n",
      "Fine-tune Pixel Refiner Epoch [202/500], Loss: 96636679.0323\n",
      "Fine-tune Pixel Refiner Epoch [203/500], Loss: 96490519.2903\n",
      "Fine-tune Pixel Refiner Epoch [204/500], Loss: 96411401.3118\n",
      "Fine-tune Pixel Refiner Epoch [205/500], Loss: 96244180.9032\n",
      "Fine-tune Pixel Refiner Epoch [206/500], Loss: 95976428.7742\n",
      "Fine-tune Pixel Refiner Epoch [207/500], Loss: 95807397.5699\n",
      "Fine-tune Pixel Refiner Epoch [208/500], Loss: 95710597.5914\n",
      "Fine-tune Pixel Refiner Epoch [209/500], Loss: 95615589.0323\n",
      "Fine-tune Pixel Refiner Epoch [210/500], Loss: 95354582.3011\n",
      "Fine-tune Pixel Refiner Epoch [211/500], Loss: 95118633.8280\n",
      "Fine-tune Pixel Refiner Epoch [212/500], Loss: 95029602.3441\n",
      "Fine-tune Pixel Refiner Epoch [213/500], Loss: 94934626.0430\n",
      "Fine-tune Pixel Refiner Epoch [214/500], Loss: 94674810.5269\n",
      "Fine-tune Pixel Refiner Epoch [215/500], Loss: 94573990.7312\n",
      "Fine-tune Pixel Refiner Epoch [216/500], Loss: 94319023.4409\n",
      "Fine-tune Pixel Refiner Epoch [217/500], Loss: 94151686.0860\n",
      "Fine-tune Pixel Refiner Epoch [218/500], Loss: 94046720.6452\n",
      "Fine-tune Pixel Refiner Epoch [219/500], Loss: 93795163.3118\n",
      "Fine-tune Pixel Refiner Epoch [220/500], Loss: 93659136.2796\n",
      "Fine-tune Pixel Refiner Epoch [221/500], Loss: 93516857.3118\n",
      "Fine-tune Pixel Refiner Epoch [222/500], Loss: 93526049.2903\n",
      "Fine-tune Pixel Refiner Epoch [223/500], Loss: 93193904.9247\n",
      "Fine-tune Pixel Refiner Epoch [224/500], Loss: 92988930.3656\n",
      "Fine-tune Pixel Refiner Epoch [225/500], Loss: 92931294.9677\n",
      "Fine-tune Pixel Refiner Epoch [226/500], Loss: 92632105.1828\n",
      "Fine-tune Pixel Refiner Epoch [227/500], Loss: 92505496.3656\n",
      "Fine-tune Pixel Refiner Epoch [228/500], Loss: 92324805.5269\n",
      "Fine-tune Pixel Refiner Epoch [229/500], Loss: 92302995.9355\n",
      "Fine-tune Pixel Refiner Epoch [230/500], Loss: 92103118.9462\n",
      "Fine-tune Pixel Refiner Epoch [231/500], Loss: 91899556.6667\n",
      "Fine-tune Pixel Refiner Epoch [232/500], Loss: 91746969.6129\n",
      "Fine-tune Pixel Refiner Epoch [233/500], Loss: 91512312.0860\n",
      "Fine-tune Pixel Refiner Epoch [234/500], Loss: 91430121.9785\n",
      "Fine-tune Pixel Refiner Epoch [235/500], Loss: 91154853.5054\n",
      "Fine-tune Pixel Refiner Epoch [236/500], Loss: 91043605.9785\n",
      "Fine-tune Pixel Refiner Epoch [237/500], Loss: 90876307.2151\n",
      "Fine-tune Pixel Refiner Epoch [238/500], Loss: 90676911.5484\n",
      "Fine-tune Pixel Refiner Epoch [239/500], Loss: 90807451.6989\n",
      "Fine-tune Pixel Refiner Epoch [240/500], Loss: 90440583.4624\n",
      "Fine-tune Pixel Refiner Epoch [241/500], Loss: 90170260.5591\n",
      "Fine-tune Pixel Refiner Epoch [242/500], Loss: 90096906.4516\n",
      "Fine-tune Pixel Refiner Epoch [243/500], Loss: 89906577.4409\n",
      "Fine-tune Pixel Refiner Epoch [244/500], Loss: 89763251.0108\n",
      "Fine-tune Pixel Refiner Epoch [245/500], Loss: 89607599.3763\n",
      "Fine-tune Pixel Refiner Epoch [246/500], Loss: 89456937.0753\n",
      "Fine-tune Pixel Refiner Epoch [247/500], Loss: 89185923.5376\n",
      "Fine-tune Pixel Refiner Epoch [248/500], Loss: 89111708.1505\n",
      "Fine-tune Pixel Refiner Epoch [249/500], Loss: 88954236.7312\n",
      "Fine-tune Pixel Refiner Epoch [250/500], Loss: 88789890.0430\n",
      "Fine-tune Pixel Refiner Epoch [251/500], Loss: 88651536.6452\n",
      "Fine-tune Pixel Refiner Epoch [252/500], Loss: 88520213.1183\n",
      "Fine-tune Pixel Refiner Epoch [253/500], Loss: 88358968.5161\n",
      "Fine-tune Pixel Refiner Epoch [254/500], Loss: 88122506.2366\n",
      "Fine-tune Pixel Refiner Epoch [255/500], Loss: 88006081.8495\n",
      "Fine-tune Pixel Refiner Epoch [256/500], Loss: 87826877.6559\n",
      "Fine-tune Pixel Refiner Epoch [257/500], Loss: 87574191.0968\n",
      "Fine-tune Pixel Refiner Epoch [258/500], Loss: 87438468.0538\n",
      "Fine-tune Pixel Refiner Epoch [259/500], Loss: 87306915.2688\n",
      "Fine-tune Pixel Refiner Epoch [260/500], Loss: 87206380.7742\n",
      "Fine-tune Pixel Refiner Epoch [261/500], Loss: 87050928.4731\n",
      "Fine-tune Pixel Refiner Epoch [262/500], Loss: 86951470.3441\n",
      "Fine-tune Pixel Refiner Epoch [263/500], Loss: 86671269.2043\n",
      "Fine-tune Pixel Refiner Epoch [264/500], Loss: 86500972.3656\n",
      "Fine-tune Pixel Refiner Epoch [265/500], Loss: 86380929.2688\n",
      "Fine-tune Pixel Refiner Epoch [266/500], Loss: 86234058.4946\n",
      "Fine-tune Pixel Refiner Epoch [267/500], Loss: 86146748.4301\n",
      "Fine-tune Pixel Refiner Epoch [268/500], Loss: 85869690.4301\n",
      "Fine-tune Pixel Refiner Epoch [269/500], Loss: 85764516.9462\n",
      "Fine-tune Pixel Refiner Epoch [270/500], Loss: 85540326.1505\n",
      "Fine-tune Pixel Refiner Epoch [271/500], Loss: 85394934.2581\n",
      "Fine-tune Pixel Refiner Epoch [272/500], Loss: 85288606.4731\n",
      "Fine-tune Pixel Refiner Epoch [273/500], Loss: 85120610.0215\n",
      "Fine-tune Pixel Refiner Epoch [274/500], Loss: 84917215.4624\n",
      "Fine-tune Pixel Refiner Epoch [275/500], Loss: 84781252.8602\n",
      "Fine-tune Pixel Refiner Epoch [276/500], Loss: 84578579.8925\n",
      "Fine-tune Pixel Refiner Epoch [277/500], Loss: 84530016.0645\n",
      "Fine-tune Pixel Refiner Epoch [278/500], Loss: 84279375.1828\n",
      "Fine-tune Pixel Refiner Epoch [279/500], Loss: 84330200.2366\n",
      "Fine-tune Pixel Refiner Epoch [280/500], Loss: 84165333.2043\n",
      "Fine-tune Pixel Refiner Epoch [281/500], Loss: 83825372.4409\n",
      "Fine-tune Pixel Refiner Epoch [282/500], Loss: 83764685.4194\n",
      "Fine-tune Pixel Refiner Epoch [283/500], Loss: 83558848.6022\n",
      "Fine-tune Pixel Refiner Epoch [284/500], Loss: 83347563.9355\n",
      "Fine-tune Pixel Refiner Epoch [285/500], Loss: 83414969.8065\n",
      "Fine-tune Pixel Refiner Epoch [286/500], Loss: 83130682.5376\n",
      "Fine-tune Pixel Refiner Epoch [287/500], Loss: 82907608.3441\n",
      "Fine-tune Pixel Refiner Epoch [288/500], Loss: 82907126.5806\n",
      "Fine-tune Pixel Refiner Epoch [289/500], Loss: 82678943.6344\n",
      "Fine-tune Pixel Refiner Epoch [290/500], Loss: 82532423.5269\n",
      "Fine-tune Pixel Refiner Epoch [291/500], Loss: 82304295.8710\n",
      "Fine-tune Pixel Refiner Epoch [292/500], Loss: 82330146.8172\n",
      "Fine-tune Pixel Refiner Epoch [293/500], Loss: 82077769.5054\n",
      "Fine-tune Pixel Refiner Epoch [294/500], Loss: 81876024.1720\n",
      "Fine-tune Pixel Refiner Epoch [295/500], Loss: 81819743.3978\n",
      "Fine-tune Pixel Refiner Epoch [296/500], Loss: 81554607.5054\n",
      "Fine-tune Pixel Refiner Epoch [297/500], Loss: 81437766.3871\n",
      "Fine-tune Pixel Refiner Epoch [298/500], Loss: 81310273.0108\n",
      "Fine-tune Pixel Refiner Epoch [299/500], Loss: 81207843.8925\n",
      "Fine-tune Pixel Refiner Epoch [300/500], Loss: 81023398.9032\n",
      "Fine-tune Pixel Refiner Epoch [301/500], Loss: 80904207.0968\n",
      "Fine-tune Pixel Refiner Epoch [302/500], Loss: 80708864.8387\n",
      "Fine-tune Pixel Refiner Epoch [303/500], Loss: 80604041.0538\n",
      "Fine-tune Pixel Refiner Epoch [304/500], Loss: 80407386.9892\n",
      "Fine-tune Pixel Refiner Epoch [305/500], Loss: 80350580.1720\n",
      "Fine-tune Pixel Refiner Epoch [306/500], Loss: 80147281.0538\n",
      "Fine-tune Pixel Refiner Epoch [307/500], Loss: 80138591.7419\n",
      "Fine-tune Pixel Refiner Epoch [308/500], Loss: 79874465.6989\n",
      "Fine-tune Pixel Refiner Epoch [309/500], Loss: 79721653.8065\n",
      "Fine-tune Pixel Refiner Epoch [310/500], Loss: 79604935.0108\n",
      "Fine-tune Pixel Refiner Epoch [311/500], Loss: 79488307.9140\n",
      "Fine-tune Pixel Refiner Epoch [312/500], Loss: 79332757.8280\n",
      "Fine-tune Pixel Refiner Epoch [313/500], Loss: 79196739.9355\n",
      "Fine-tune Pixel Refiner Epoch [314/500], Loss: 79040174.3871\n",
      "Fine-tune Pixel Refiner Epoch [315/500], Loss: 79017095.4194\n",
      "Fine-tune Pixel Refiner Epoch [316/500], Loss: 78989085.0108\n",
      "Fine-tune Pixel Refiner Epoch [317/500], Loss: 78673990.3656\n",
      "Fine-tune Pixel Refiner Epoch [318/500], Loss: 78537116.1720\n",
      "Fine-tune Pixel Refiner Epoch [319/500], Loss: 78479862.7742\n",
      "Fine-tune Pixel Refiner Epoch [320/500], Loss: 78284166.2366\n",
      "Fine-tune Pixel Refiner Epoch [321/500], Loss: 78217170.5376\n",
      "Fine-tune Pixel Refiner Epoch [322/500], Loss: 78007071.2903\n",
      "Fine-tune Pixel Refiner Epoch [323/500], Loss: 77920199.0753\n",
      "Fine-tune Pixel Refiner Epoch [324/500], Loss: 77758443.5161\n",
      "Fine-tune Pixel Refiner Epoch [325/500], Loss: 77895109.3118\n",
      "Fine-tune Pixel Refiner Epoch [326/500], Loss: 77587747.6344\n",
      "Fine-tune Pixel Refiner Epoch [327/500], Loss: 77426360.6667\n",
      "Fine-tune Pixel Refiner Epoch [328/500], Loss: 77353275.8495\n",
      "Fine-tune Pixel Refiner Epoch [329/500], Loss: 77167024.8602\n",
      "Fine-tune Pixel Refiner Epoch [330/500], Loss: 77051867.4409\n",
      "Fine-tune Pixel Refiner Epoch [331/500], Loss: 76966397.1398\n",
      "Fine-tune Pixel Refiner Epoch [332/500], Loss: 76826107.5269\n",
      "Fine-tune Pixel Refiner Epoch [333/500], Loss: 76708341.6989\n",
      "Fine-tune Pixel Refiner Epoch [334/500], Loss: 76866994.2151\n",
      "Fine-tune Pixel Refiner Epoch [335/500], Loss: 76585562.6882\n",
      "Fine-tune Pixel Refiner Epoch [336/500], Loss: 76385967.6344\n",
      "Fine-tune Pixel Refiner Epoch [337/500], Loss: 76312933.7634\n",
      "Fine-tune Pixel Refiner Epoch [338/500], Loss: 76260910.1075\n",
      "Fine-tune Pixel Refiner Epoch [339/500], Loss: 76249665.9140\n",
      "Fine-tune Pixel Refiner Epoch [340/500], Loss: 76044479.2688\n",
      "Fine-tune Pixel Refiner Epoch [341/500], Loss: 76080110.8817\n",
      "Fine-tune Pixel Refiner Epoch [342/500], Loss: 75868135.2258\n",
      "Fine-tune Pixel Refiner Epoch [343/500], Loss: 75855847.8495\n",
      "Fine-tune Pixel Refiner Epoch [344/500], Loss: 75777691.9570\n",
      "Fine-tune Pixel Refiner Epoch [345/500], Loss: 75464772.0968\n",
      "Fine-tune Pixel Refiner Epoch [346/500], Loss: 75359483.4301\n",
      "Fine-tune Pixel Refiner Epoch [347/500], Loss: 75293893.3118\n",
      "Fine-tune Pixel Refiner Epoch [348/500], Loss: 75228480.5376\n",
      "Fine-tune Pixel Refiner Epoch [349/500], Loss: 75104878.4946\n",
      "Fine-tune Pixel Refiner Epoch [350/500], Loss: 74997749.8495\n",
      "Fine-tune Pixel Refiner Epoch [351/500], Loss: 74975115.4839\n",
      "Fine-tune Pixel Refiner Epoch [352/500], Loss: 74919511.9140\n",
      "Fine-tune Pixel Refiner Epoch [353/500], Loss: 74724031.9355\n",
      "Fine-tune Pixel Refiner Epoch [354/500], Loss: 74640183.9462\n",
      "Fine-tune Pixel Refiner Epoch [355/500], Loss: 74587671.0323\n",
      "Fine-tune Pixel Refiner Epoch [356/500], Loss: 74483016.0860\n",
      "Fine-tune Pixel Refiner Epoch [357/500], Loss: 74382742.7312\n",
      "Fine-tune Pixel Refiner Epoch [358/500], Loss: 74309488.5376\n",
      "Fine-tune Pixel Refiner Epoch [359/500], Loss: 74225376.8172\n",
      "Fine-tune Pixel Refiner Epoch [360/500], Loss: 74157862.2581\n",
      "Fine-tune Pixel Refiner Epoch [361/500], Loss: 74001539.9355\n",
      "Fine-tune Pixel Refiner Epoch [362/500], Loss: 73985721.6344\n",
      "Fine-tune Pixel Refiner Epoch [363/500], Loss: 73845354.9892\n",
      "Fine-tune Pixel Refiner Epoch [364/500], Loss: 73792441.5484\n",
      "Fine-tune Pixel Refiner Epoch [365/500], Loss: 73651456.8817\n",
      "Fine-tune Pixel Refiner Epoch [366/500], Loss: 73569421.8925\n",
      "Fine-tune Pixel Refiner Epoch [367/500], Loss: 73470184.5806\n",
      "Fine-tune Pixel Refiner Epoch [368/500], Loss: 73483591.6989\n",
      "Fine-tune Pixel Refiner Epoch [369/500], Loss: 73382657.3118\n",
      "Fine-tune Pixel Refiner Epoch [370/500], Loss: 73266102.7527\n",
      "Fine-tune Pixel Refiner Epoch [371/500], Loss: 73196283.3978\n",
      "Fine-tune Pixel Refiner Epoch [372/500], Loss: 73041490.6022\n",
      "Fine-tune Pixel Refiner Epoch [373/500], Loss: 73208908.4516\n",
      "Fine-tune Pixel Refiner Epoch [374/500], Loss: 72893701.1613\n",
      "Fine-tune Pixel Refiner Epoch [375/500], Loss: 72824469.6774\n",
      "Fine-tune Pixel Refiner Epoch [376/500], Loss: 72767912.9892\n",
      "Fine-tune Pixel Refiner Epoch [377/500], Loss: 72677016.2366\n",
      "Fine-tune Pixel Refiner Epoch [378/500], Loss: 72637127.6129\n",
      "Fine-tune Pixel Refiner Epoch [379/500], Loss: 72655256.6237\n",
      "Fine-tune Pixel Refiner Epoch [380/500], Loss: 72595310.1505\n",
      "Fine-tune Pixel Refiner Epoch [381/500], Loss: 72489642.3226\n",
      "Fine-tune Pixel Refiner Epoch [382/500], Loss: 72279135.7849\n",
      "Fine-tune Pixel Refiner Epoch [383/500], Loss: 72286871.8495\n",
      "Fine-tune Pixel Refiner Epoch [384/500], Loss: 72106014.7634\n",
      "Fine-tune Pixel Refiner Epoch [385/500], Loss: 72015637.4839\n",
      "Fine-tune Pixel Refiner Epoch [386/500], Loss: 71975992.3226\n",
      "Fine-tune Pixel Refiner Epoch [387/500], Loss: 71947710.7957\n",
      "Fine-tune Pixel Refiner Epoch [388/500], Loss: 71826508.4194\n",
      "Fine-tune Pixel Refiner Epoch [389/500], Loss: 71701115.3763\n",
      "Fine-tune Pixel Refiner Epoch [390/500], Loss: 71839718.3441\n",
      "Fine-tune Pixel Refiner Epoch [391/500], Loss: 71679475.7634\n",
      "Fine-tune Pixel Refiner Epoch [392/500], Loss: 71511721.3978\n",
      "Fine-tune Pixel Refiner Epoch [393/500], Loss: 71435625.4731\n",
      "Fine-tune Pixel Refiner Epoch [394/500], Loss: 71367458.7312\n",
      "Fine-tune Pixel Refiner Epoch [395/500], Loss: 71395429.6989\n",
      "Fine-tune Pixel Refiner Epoch [396/500], Loss: 71278647.8280\n",
      "Fine-tune Pixel Refiner Epoch [397/500], Loss: 71187567.1613\n",
      "Fine-tune Pixel Refiner Epoch [398/500], Loss: 71074765.5484\n",
      "Fine-tune Pixel Refiner Epoch [399/500], Loss: 71057722.8602\n",
      "Fine-tune Pixel Refiner Epoch [400/500], Loss: 70939848.0215\n",
      "Fine-tune Pixel Refiner Epoch [401/500], Loss: 70869899.4839\n",
      "Fine-tune Pixel Refiner Epoch [402/500], Loss: 70794423.5914\n",
      "Fine-tune Pixel Refiner Epoch [403/500], Loss: 70776488.1720\n",
      "Fine-tune Pixel Refiner Epoch [404/500], Loss: 70693937.9677\n",
      "Fine-tune Pixel Refiner Epoch [405/500], Loss: 70632272.4086\n",
      "Fine-tune Pixel Refiner Epoch [406/500], Loss: 70549936.4086\n",
      "Fine-tune Pixel Refiner Epoch [407/500], Loss: 70513748.5699\n",
      "Fine-tune Pixel Refiner Epoch [408/500], Loss: 70541102.9677\n",
      "Fine-tune Pixel Refiner Epoch [409/500], Loss: 70367913.5054\n",
      "Fine-tune Pixel Refiner Epoch [410/500], Loss: 70408960.7957\n",
      "Fine-tune Pixel Refiner Epoch [411/500], Loss: 70255222.2581\n",
      "Fine-tune Pixel Refiner Epoch [412/500], Loss: 70234465.2903\n",
      "Fine-tune Pixel Refiner Epoch [413/500], Loss: 70130419.5914\n",
      "Fine-tune Pixel Refiner Epoch [414/500], Loss: 70208651.2473\n",
      "Fine-tune Pixel Refiner Epoch [415/500], Loss: 70127785.6774\n",
      "Fine-tune Pixel Refiner Epoch [416/500], Loss: 69977388.9247\n",
      "Fine-tune Pixel Refiner Epoch [417/500], Loss: 69938410.4731\n",
      "Fine-tune Pixel Refiner Epoch [418/500], Loss: 70036198.5376\n",
      "Fine-tune Pixel Refiner Epoch [419/500], Loss: 69804276.9462\n",
      "Fine-tune Pixel Refiner Epoch [420/500], Loss: 69914796.4301\n",
      "Fine-tune Pixel Refiner Epoch [421/500], Loss: 69723562.7957\n",
      "Fine-tune Pixel Refiner Epoch [422/500], Loss: 69672070.4946\n",
      "Fine-tune Pixel Refiner Epoch [423/500], Loss: 69615508.6882\n",
      "Fine-tune Pixel Refiner Epoch [424/500], Loss: 69580475.9785\n",
      "Fine-tune Pixel Refiner Epoch [425/500], Loss: 69598432.9677\n",
      "Fine-tune Pixel Refiner Epoch [426/500], Loss: 69669436.9032\n",
      "Fine-tune Pixel Refiner Epoch [427/500], Loss: 69395248.3441\n",
      "Fine-tune Pixel Refiner Epoch [428/500], Loss: 69416031.5054\n",
      "Fine-tune Pixel Refiner Epoch [429/500], Loss: 69397414.0215\n",
      "Fine-tune Pixel Refiner Epoch [430/500], Loss: 69246250.5806\n",
      "Fine-tune Pixel Refiner Epoch [431/500], Loss: 69236193.4839\n",
      "Fine-tune Pixel Refiner Epoch [432/500], Loss: 69232359.4409\n",
      "Fine-tune Pixel Refiner Epoch [433/500], Loss: 69242033.0968\n",
      "Fine-tune Pixel Refiner Epoch [434/500], Loss: 69132061.5914\n",
      "Fine-tune Pixel Refiner Epoch [435/500], Loss: 69066820.5806\n",
      "Fine-tune Pixel Refiner Epoch [436/500], Loss: 69109136.3548\n",
      "Fine-tune Pixel Refiner Epoch [437/500], Loss: 68951672.8387\n",
      "Fine-tune Pixel Refiner Epoch [438/500], Loss: 68925652.1075\n",
      "Fine-tune Pixel Refiner Epoch [439/500], Loss: 68860009.6989\n",
      "Fine-tune Pixel Refiner Epoch [440/500], Loss: 68856631.5054\n",
      "Fine-tune Pixel Refiner Epoch [441/500], Loss: 68789068.0968\n",
      "Fine-tune Pixel Refiner Epoch [442/500], Loss: 68783767.2473\n",
      "Fine-tune Pixel Refiner Epoch [443/500], Loss: 68740389.1183\n",
      "Fine-tune Pixel Refiner Epoch [444/500], Loss: 68715059.7204\n",
      "Fine-tune Pixel Refiner Epoch [445/500], Loss: 68631849.5054\n",
      "Fine-tune Pixel Refiner Epoch [446/500], Loss: 68675960.7527\n",
      "Fine-tune Pixel Refiner Epoch [447/500], Loss: 68712348.3333\n",
      "Fine-tune Pixel Refiner Epoch [448/500], Loss: 68590644.9462\n",
      "Fine-tune Pixel Refiner Epoch [449/500], Loss: 68480057.1398\n",
      "Fine-tune Pixel Refiner Epoch [450/500], Loss: 68485340.0000\n",
      "Fine-tune Pixel Refiner Epoch [451/500], Loss: 68420325.5914\n",
      "Fine-tune Pixel Refiner Epoch [452/500], Loss: 68468196.2796\n",
      "Fine-tune Pixel Refiner Epoch [453/500], Loss: 68404918.7097\n",
      "Fine-tune Pixel Refiner Epoch [454/500], Loss: 68365634.1505\n",
      "Fine-tune Pixel Refiner Epoch [455/500], Loss: 68338482.0215\n",
      "Fine-tune Pixel Refiner Epoch [456/500], Loss: 68298934.8387\n",
      "Fine-tune Pixel Refiner Epoch [457/500], Loss: 68240138.3871\n",
      "Fine-tune Pixel Refiner Epoch [458/500], Loss: 68253305.5699\n",
      "Fine-tune Pixel Refiner Epoch [459/500], Loss: 68315762.0860\n",
      "Fine-tune Pixel Refiner Epoch [460/500], Loss: 68213735.9785\n",
      "Fine-tune Pixel Refiner Epoch [461/500], Loss: 68114944.0215\n",
      "Fine-tune Pixel Refiner Epoch [462/500], Loss: 68099090.1075\n",
      "Fine-tune Pixel Refiner Epoch [463/500], Loss: 68135402.0860\n",
      "Fine-tune Pixel Refiner Epoch [464/500], Loss: 68121070.8817\n",
      "Fine-tune Pixel Refiner Epoch [465/500], Loss: 68029911.3333\n",
      "Fine-tune Pixel Refiner Epoch [466/500], Loss: 68008516.2581\n",
      "Fine-tune Pixel Refiner Epoch [467/500], Loss: 67993405.9355\n",
      "Fine-tune Pixel Refiner Epoch [468/500], Loss: 67912715.8710\n",
      "Fine-tune Pixel Refiner Epoch [469/500], Loss: 67977553.8925\n",
      "Fine-tune Pixel Refiner Epoch [470/500], Loss: 67898602.8172\n",
      "Fine-tune Pixel Refiner Epoch [471/500], Loss: 67860822.0215\n",
      "Fine-tune Pixel Refiner Epoch [472/500], Loss: 67824988.6452\n",
      "Fine-tune Pixel Refiner Epoch [473/500], Loss: 67859989.7634\n",
      "Fine-tune Pixel Refiner Epoch [474/500], Loss: 67814474.4086\n",
      "Fine-tune Pixel Refiner Epoch [475/500], Loss: 67840168.9892\n",
      "Fine-tune Pixel Refiner Epoch [476/500], Loss: 67717554.9462\n",
      "Fine-tune Pixel Refiner Epoch [477/500], Loss: 67726991.3548\n",
      "Fine-tune Pixel Refiner Epoch [478/500], Loss: 67705203.4946\n",
      "Fine-tune Pixel Refiner Epoch [479/500], Loss: 67762241.6559\n",
      "Fine-tune Pixel Refiner Epoch [480/500], Loss: 67668038.8387\n",
      "Fine-tune Pixel Refiner Epoch [481/500], Loss: 67666761.3978\n",
      "Fine-tune Pixel Refiner Epoch [482/500], Loss: 67611647.3118\n",
      "Fine-tune Pixel Refiner Epoch [483/500], Loss: 67603764.3871\n",
      "Fine-tune Pixel Refiner Epoch [484/500], Loss: 67540276.4086\n",
      "Fine-tune Pixel Refiner Epoch [485/500], Loss: 67537480.3226\n",
      "Fine-tune Pixel Refiner Epoch [486/500], Loss: 67678647.9140\n",
      "Fine-tune Pixel Refiner Epoch [487/500], Loss: 67537808.0645\n",
      "Fine-tune Pixel Refiner Epoch [488/500], Loss: 67495910.9462\n",
      "Fine-tune Pixel Refiner Epoch [489/500], Loss: 67536565.3333\n",
      "Fine-tune Pixel Refiner Epoch [490/500], Loss: 67461622.0645\n",
      "Fine-tune Pixel Refiner Epoch [491/500], Loss: 67411890.6237\n",
      "Fine-tune Pixel Refiner Epoch [492/500], Loss: 67424097.5699\n",
      "Fine-tune Pixel Refiner Epoch [493/500], Loss: 67382905.4839\n",
      "Fine-tune Pixel Refiner Epoch [494/500], Loss: 67412050.6667\n",
      "Fine-tune Pixel Refiner Epoch [495/500], Loss: 67359015.0108\n",
      "Fine-tune Pixel Refiner Epoch [496/500], Loss: 67350526.0645\n",
      "Fine-tune Pixel Refiner Epoch [497/500], Loss: 67334447.2688\n",
      "Fine-tune Pixel Refiner Epoch [498/500], Loss: 67326490.5591\n",
      "Fine-tune Pixel Refiner Epoch [499/500], Loss: 67295712.6882\n",
      "Fine-tune Pixel Refiner Epoch [500/500], Loss: 67413165.4409\n",
      "Pixel Refiner fine-tuned and saved successfully!\n"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T02:50:14.704Z",
     "start_time": "2025-03-30T02:50:14.699324Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "\n",
    "refiner_model.load_state_dict(torch.load('pixel_refiner_finetuned.pth', map_location=device))\n",
    "refiner_model.eval()\n"
   ],
   "id": "caed264286bafe0e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PixelRefiner(\n",
       "  (conv_layers): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): AdaptiveAvgPool2d(output_size=(4, 4))\n",
       "  )\n",
       "  (fc_attention): Sequential(\n",
       "    (0): Linear(in_features=513, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=1, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T03:00:49.152597Z",
     "start_time": "2025-03-30T03:00:49.149300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.save(refiner_model.state_dict(), 'pixel_refiner_finetuned.pth')\n",
    "print(\"Pixel Refiner fine-tuned and saved successfully!\")"
   ],
   "id": "ff4f1f262ed9da35",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel Refiner fine-tuned and saved successfully!\n"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T03:11:16.924696Z",
     "start_time": "2025-03-30T03:01:15.961637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "refiner_model.load_state_dict(torch.load('pixel_refiner_finetuned.pth', map_location=device))\n",
    "refiner_model.train()\n",
    "\n",
    "criterion_refiner = nn.MSELoss()\n",
    "optimizer_refiner = torch.optim.Adam(refiner_model.parameters(), lr=1e-5)  # use lower LR for fine-tuning\n",
    "\n",
    "fine_tune_epochs = 500\n",
    "for epoch in range(fine_tune_epochs):\n",
    "    total_loss = 0\n",
    "    for imgs, initial_preds, gt_pixels in updated_loader:\n",
    "        imgs, initial_preds, gt_pixels = imgs.to(device), initial_preds.to(device), gt_pixels.to(device)\n",
    "\n",
    "        optimizer_refiner.zero_grad()\n",
    "        refined_output = refiner_model(imgs, initial_preds)\n",
    "        loss = criterion_refiner(refined_output, gt_pixels)\n",
    "        loss.backward()\n",
    "        optimizer_refiner.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(updated_loader)\n",
    "    print(f\"Fine-tune Pixel Refiner Epoch [{epoch+1}/{fine_tune_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Save fine-tuned Pixel Refiner clearly\n",
    "torch.save(refiner_model.state_dict(), 'pixel_refiner_finetuned.pth')\n",
    "print(\"Pixel Refiner fine-tuned and saved successfully!\")\n"
   ],
   "id": "d69dd81deacf87f9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Pixel Refiner Epoch [1/500], Loss: 67394513.4839\n",
      "Fine-tune Pixel Refiner Epoch [2/500], Loss: 67335424.3011\n",
      "Fine-tune Pixel Refiner Epoch [3/500], Loss: 67237186.3656\n",
      "Fine-tune Pixel Refiner Epoch [4/500], Loss: 67245135.1183\n",
      "Fine-tune Pixel Refiner Epoch [5/500], Loss: 67308916.3656\n",
      "Fine-tune Pixel Refiner Epoch [6/500], Loss: 67252974.8387\n",
      "Fine-tune Pixel Refiner Epoch [7/500], Loss: 67178976.9032\n",
      "Fine-tune Pixel Refiner Epoch [8/500], Loss: 67238008.5591\n",
      "Fine-tune Pixel Refiner Epoch [9/500], Loss: 67201816.5914\n",
      "Fine-tune Pixel Refiner Epoch [10/500], Loss: 67256056.1290\n",
      "Fine-tune Pixel Refiner Epoch [11/500], Loss: 67155746.2151\n",
      "Fine-tune Pixel Refiner Epoch [12/500], Loss: 67121633.0968\n",
      "Fine-tune Pixel Refiner Epoch [13/500], Loss: 67202710.8602\n",
      "Fine-tune Pixel Refiner Epoch [14/500], Loss: 67175102.8387\n",
      "Fine-tune Pixel Refiner Epoch [15/500], Loss: 67177877.0753\n",
      "Fine-tune Pixel Refiner Epoch [16/500], Loss: 67078740.3871\n",
      "Fine-tune Pixel Refiner Epoch [17/500], Loss: 67153508.3226\n",
      "Fine-tune Pixel Refiner Epoch [18/500], Loss: 67110589.6989\n",
      "Fine-tune Pixel Refiner Epoch [19/500], Loss: 67116241.2473\n",
      "Fine-tune Pixel Refiner Epoch [20/500], Loss: 67074977.9140\n",
      "Fine-tune Pixel Refiner Epoch [21/500], Loss: 67060329.6129\n",
      "Fine-tune Pixel Refiner Epoch [22/500], Loss: 67115963.6344\n",
      "Fine-tune Pixel Refiner Epoch [23/500], Loss: 67026713.0323\n",
      "Fine-tune Pixel Refiner Epoch [24/500], Loss: 67098120.5591\n",
      "Fine-tune Pixel Refiner Epoch [25/500], Loss: 67064975.5914\n",
      "Fine-tune Pixel Refiner Epoch [26/500], Loss: 67080881.4194\n",
      "Fine-tune Pixel Refiner Epoch [27/500], Loss: 66953282.4086\n",
      "Fine-tune Pixel Refiner Epoch [28/500], Loss: 66953141.6129\n",
      "Fine-tune Pixel Refiner Epoch [29/500], Loss: 66970991.3978\n",
      "Fine-tune Pixel Refiner Epoch [30/500], Loss: 67031957.0538\n",
      "Fine-tune Pixel Refiner Epoch [31/500], Loss: 66945987.4624\n",
      "Fine-tune Pixel Refiner Epoch [32/500], Loss: 66905999.8172\n",
      "Fine-tune Pixel Refiner Epoch [33/500], Loss: 66923387.9570\n",
      "Fine-tune Pixel Refiner Epoch [34/500], Loss: 66951886.0645\n",
      "Fine-tune Pixel Refiner Epoch [35/500], Loss: 66908466.1720\n",
      "Fine-tune Pixel Refiner Epoch [36/500], Loss: 66925970.4731\n",
      "Fine-tune Pixel Refiner Epoch [37/500], Loss: 66956635.6989\n",
      "Fine-tune Pixel Refiner Epoch [38/500], Loss: 66990509.5484\n",
      "Fine-tune Pixel Refiner Epoch [39/500], Loss: 66879214.5161\n",
      "Fine-tune Pixel Refiner Epoch [40/500], Loss: 66853935.4194\n",
      "Fine-tune Pixel Refiner Epoch [41/500], Loss: 66843995.8925\n",
      "Fine-tune Pixel Refiner Epoch [42/500], Loss: 66856508.3441\n",
      "Fine-tune Pixel Refiner Epoch [43/500], Loss: 66838558.4516\n",
      "Fine-tune Pixel Refiner Epoch [44/500], Loss: 66906323.9462\n",
      "Fine-tune Pixel Refiner Epoch [45/500], Loss: 66892526.1935\n",
      "Fine-tune Pixel Refiner Epoch [46/500], Loss: 66802763.8710\n",
      "Fine-tune Pixel Refiner Epoch [47/500], Loss: 66826512.0860\n",
      "Fine-tune Pixel Refiner Epoch [48/500], Loss: 66785432.5806\n",
      "Fine-tune Pixel Refiner Epoch [49/500], Loss: 66781154.8172\n",
      "Fine-tune Pixel Refiner Epoch [50/500], Loss: 66771256.9247\n",
      "Fine-tune Pixel Refiner Epoch [51/500], Loss: 66755121.1828\n",
      "Fine-tune Pixel Refiner Epoch [52/500], Loss: 66766828.6022\n",
      "Fine-tune Pixel Refiner Epoch [53/500], Loss: 66807984.7527\n",
      "Fine-tune Pixel Refiner Epoch [54/500], Loss: 66881552.1398\n",
      "Fine-tune Pixel Refiner Epoch [55/500], Loss: 66754823.5054\n",
      "Fine-tune Pixel Refiner Epoch [56/500], Loss: 66725938.3226\n",
      "Fine-tune Pixel Refiner Epoch [57/500], Loss: 66719783.0968\n",
      "Fine-tune Pixel Refiner Epoch [58/500], Loss: 66714801.4624\n",
      "Fine-tune Pixel Refiner Epoch [59/500], Loss: 66734291.7849\n",
      "Fine-tune Pixel Refiner Epoch [60/500], Loss: 66777849.5914\n",
      "Fine-tune Pixel Refiner Epoch [61/500], Loss: 66771347.7204\n",
      "Fine-tune Pixel Refiner Epoch [62/500], Loss: 66730397.8495\n",
      "Fine-tune Pixel Refiner Epoch [63/500], Loss: 66692560.8817\n",
      "Fine-tune Pixel Refiner Epoch [64/500], Loss: 66842683.8495\n",
      "Fine-tune Pixel Refiner Epoch [65/500], Loss: 66689448.2366\n",
      "Fine-tune Pixel Refiner Epoch [66/500], Loss: 66681954.4731\n",
      "Fine-tune Pixel Refiner Epoch [67/500], Loss: 66673335.2473\n",
      "Fine-tune Pixel Refiner Epoch [68/500], Loss: 66656630.7097\n",
      "Fine-tune Pixel Refiner Epoch [69/500], Loss: 66740132.5376\n",
      "Fine-tune Pixel Refiner Epoch [70/500], Loss: 66645848.4301\n",
      "Fine-tune Pixel Refiner Epoch [71/500], Loss: 66643171.0968\n",
      "Fine-tune Pixel Refiner Epoch [72/500], Loss: 66653488.7527\n",
      "Fine-tune Pixel Refiner Epoch [73/500], Loss: 66619794.4086\n",
      "Fine-tune Pixel Refiner Epoch [74/500], Loss: 66659815.7204\n",
      "Fine-tune Pixel Refiner Epoch [75/500], Loss: 66629534.5591\n",
      "Fine-tune Pixel Refiner Epoch [76/500], Loss: 66603330.4086\n",
      "Fine-tune Pixel Refiner Epoch [77/500], Loss: 66630616.1505\n",
      "Fine-tune Pixel Refiner Epoch [78/500], Loss: 66714099.5054\n",
      "Fine-tune Pixel Refiner Epoch [79/500], Loss: 66607917.4194\n",
      "Fine-tune Pixel Refiner Epoch [80/500], Loss: 66574125.2903\n",
      "Fine-tune Pixel Refiner Epoch [81/500], Loss: 66574515.6774\n",
      "Fine-tune Pixel Refiner Epoch [82/500], Loss: 66701156.1505\n",
      "Fine-tune Pixel Refiner Epoch [83/500], Loss: 66614032.5806\n",
      "Fine-tune Pixel Refiner Epoch [84/500], Loss: 66618263.2043\n",
      "Fine-tune Pixel Refiner Epoch [85/500], Loss: 66634566.2796\n",
      "Fine-tune Pixel Refiner Epoch [86/500], Loss: 66602347.9570\n",
      "Fine-tune Pixel Refiner Epoch [87/500], Loss: 66612619.4839\n",
      "Fine-tune Pixel Refiner Epoch [88/500], Loss: 66568127.6774\n",
      "Fine-tune Pixel Refiner Epoch [89/500], Loss: 66547827.5699\n",
      "Fine-tune Pixel Refiner Epoch [90/500], Loss: 66698586.2581\n",
      "Fine-tune Pixel Refiner Epoch [91/500], Loss: 66595523.6344\n",
      "Fine-tune Pixel Refiner Epoch [92/500], Loss: 66592031.6774\n",
      "Fine-tune Pixel Refiner Epoch [93/500], Loss: 66539227.2473\n",
      "Fine-tune Pixel Refiner Epoch [94/500], Loss: 66524906.9892\n",
      "Fine-tune Pixel Refiner Epoch [95/500], Loss: 66608933.6129\n",
      "Fine-tune Pixel Refiner Epoch [96/500], Loss: 66493580.9462\n",
      "Fine-tune Pixel Refiner Epoch [97/500], Loss: 66546360.3871\n",
      "Fine-tune Pixel Refiner Epoch [98/500], Loss: 66595745.5914\n",
      "Fine-tune Pixel Refiner Epoch [99/500], Loss: 66526850.8602\n",
      "Fine-tune Pixel Refiner Epoch [100/500], Loss: 66505488.8817\n",
      "Fine-tune Pixel Refiner Epoch [101/500], Loss: 66527049.8495\n",
      "Fine-tune Pixel Refiner Epoch [102/500], Loss: 66495511.4624\n",
      "Fine-tune Pixel Refiner Epoch [103/500], Loss: 66471719.0215\n",
      "Fine-tune Pixel Refiner Epoch [104/500], Loss: 66562260.3441\n",
      "Fine-tune Pixel Refiner Epoch [105/500], Loss: 66601367.6774\n",
      "Fine-tune Pixel Refiner Epoch [106/500], Loss: 66494711.6129\n",
      "Fine-tune Pixel Refiner Epoch [107/500], Loss: 66507501.4624\n",
      "Fine-tune Pixel Refiner Epoch [108/500], Loss: 66589324.4086\n",
      "Fine-tune Pixel Refiner Epoch [109/500], Loss: 66451577.3333\n",
      "Fine-tune Pixel Refiner Epoch [110/500], Loss: 66476372.0000\n",
      "Fine-tune Pixel Refiner Epoch [111/500], Loss: 66547908.2688\n",
      "Fine-tune Pixel Refiner Epoch [112/500], Loss: 66467063.5699\n",
      "Fine-tune Pixel Refiner Epoch [113/500], Loss: 66520080.7742\n",
      "Fine-tune Pixel Refiner Epoch [114/500], Loss: 66453886.8280\n",
      "Fine-tune Pixel Refiner Epoch [115/500], Loss: 66432238.1505\n",
      "Fine-tune Pixel Refiner Epoch [116/500], Loss: 66439829.6989\n",
      "Fine-tune Pixel Refiner Epoch [117/500], Loss: 66442219.1183\n",
      "Fine-tune Pixel Refiner Epoch [118/500], Loss: 66520064.2366\n",
      "Fine-tune Pixel Refiner Epoch [119/500], Loss: 66418062.0860\n",
      "Fine-tune Pixel Refiner Epoch [120/500], Loss: 66410800.1290\n",
      "Fine-tune Pixel Refiner Epoch [121/500], Loss: 66424340.4946\n",
      "Fine-tune Pixel Refiner Epoch [122/500], Loss: 66432214.4731\n",
      "Fine-tune Pixel Refiner Epoch [123/500], Loss: 66376718.8817\n",
      "Fine-tune Pixel Refiner Epoch [124/500], Loss: 66436515.8710\n",
      "Fine-tune Pixel Refiner Epoch [125/500], Loss: 66456974.1720\n",
      "Fine-tune Pixel Refiner Epoch [126/500], Loss: 66389229.7849\n",
      "Fine-tune Pixel Refiner Epoch [127/500], Loss: 66462140.4086\n",
      "Fine-tune Pixel Refiner Epoch [128/500], Loss: 66412743.1613\n",
      "Fine-tune Pixel Refiner Epoch [129/500], Loss: 66399765.0645\n",
      "Fine-tune Pixel Refiner Epoch [130/500], Loss: 66414081.1613\n",
      "Fine-tune Pixel Refiner Epoch [131/500], Loss: 66397211.9140\n",
      "Fine-tune Pixel Refiner Epoch [132/500], Loss: 66443622.0645\n",
      "Fine-tune Pixel Refiner Epoch [133/500], Loss: 66365707.7419\n",
      "Fine-tune Pixel Refiner Epoch [134/500], Loss: 66434711.2473\n",
      "Fine-tune Pixel Refiner Epoch [135/500], Loss: 66357005.8495\n",
      "Fine-tune Pixel Refiner Epoch [136/500], Loss: 66355037.8495\n",
      "Fine-tune Pixel Refiner Epoch [137/500], Loss: 66383835.9355\n",
      "Fine-tune Pixel Refiner Epoch [138/500], Loss: 66454005.8280\n",
      "Fine-tune Pixel Refiner Epoch [139/500], Loss: 66397557.5484\n",
      "Fine-tune Pixel Refiner Epoch [140/500], Loss: 66359187.6989\n",
      "Fine-tune Pixel Refiner Epoch [141/500], Loss: 66330731.7634\n",
      "Fine-tune Pixel Refiner Epoch [142/500], Loss: 66330688.3226\n",
      "Fine-tune Pixel Refiner Epoch [143/500], Loss: 66326032.6667\n",
      "Fine-tune Pixel Refiner Epoch [144/500], Loss: 66351996.1290\n",
      "Fine-tune Pixel Refiner Epoch [145/500], Loss: 66321004.9892\n",
      "Fine-tune Pixel Refiner Epoch [146/500], Loss: 66368970.0645\n",
      "Fine-tune Pixel Refiner Epoch [147/500], Loss: 66398386.7742\n",
      "Fine-tune Pixel Refiner Epoch [148/500], Loss: 66312050.9892\n",
      "Fine-tune Pixel Refiner Epoch [149/500], Loss: 66355910.0430\n",
      "Fine-tune Pixel Refiner Epoch [150/500], Loss: 66322496.7527\n",
      "Fine-tune Pixel Refiner Epoch [151/500], Loss: 66293653.4194\n",
      "Fine-tune Pixel Refiner Epoch [152/500], Loss: 66355741.4624\n",
      "Fine-tune Pixel Refiner Epoch [153/500], Loss: 66466488.4086\n",
      "Fine-tune Pixel Refiner Epoch [154/500], Loss: 66300703.1398\n",
      "Fine-tune Pixel Refiner Epoch [155/500], Loss: 66333396.1290\n",
      "Fine-tune Pixel Refiner Epoch [156/500], Loss: 66446285.6344\n",
      "Fine-tune Pixel Refiner Epoch [157/500], Loss: 66460109.1613\n",
      "Fine-tune Pixel Refiner Epoch [158/500], Loss: 66351161.4839\n",
      "Fine-tune Pixel Refiner Epoch [159/500], Loss: 66275981.5914\n",
      "Fine-tune Pixel Refiner Epoch [160/500], Loss: 66360598.6452\n",
      "Fine-tune Pixel Refiner Epoch [161/500], Loss: 66264688.9570\n",
      "Fine-tune Pixel Refiner Epoch [162/500], Loss: 66329576.1075\n",
      "Fine-tune Pixel Refiner Epoch [163/500], Loss: 66352964.7742\n",
      "Fine-tune Pixel Refiner Epoch [164/500], Loss: 66263918.3441\n",
      "Fine-tune Pixel Refiner Epoch [165/500], Loss: 66331325.1828\n",
      "Fine-tune Pixel Refiner Epoch [166/500], Loss: 66342719.7849\n",
      "Fine-tune Pixel Refiner Epoch [167/500], Loss: 66240864.2151\n",
      "Fine-tune Pixel Refiner Epoch [168/500], Loss: 66292743.4839\n",
      "Fine-tune Pixel Refiner Epoch [169/500], Loss: 66352326.5161\n",
      "Fine-tune Pixel Refiner Epoch [170/500], Loss: 66268084.3656\n",
      "Fine-tune Pixel Refiner Epoch [171/500], Loss: 66347318.6452\n",
      "Fine-tune Pixel Refiner Epoch [172/500], Loss: 66246718.2796\n",
      "Fine-tune Pixel Refiner Epoch [173/500], Loss: 66329753.0753\n",
      "Fine-tune Pixel Refiner Epoch [174/500], Loss: 66328521.5699\n",
      "Fine-tune Pixel Refiner Epoch [175/500], Loss: 66329828.4086\n",
      "Fine-tune Pixel Refiner Epoch [176/500], Loss: 66293586.0430\n",
      "Fine-tune Pixel Refiner Epoch [177/500], Loss: 66236077.7419\n",
      "Fine-tune Pixel Refiner Epoch [178/500], Loss: 66250507.8065\n",
      "Fine-tune Pixel Refiner Epoch [179/500], Loss: 66269448.4516\n",
      "Fine-tune Pixel Refiner Epoch [180/500], Loss: 66200617.7849\n",
      "Fine-tune Pixel Refiner Epoch [181/500], Loss: 66207312.6882\n",
      "Fine-tune Pixel Refiner Epoch [182/500], Loss: 66244920.1075\n",
      "Fine-tune Pixel Refiner Epoch [183/500], Loss: 66276381.9140\n",
      "Fine-tune Pixel Refiner Epoch [184/500], Loss: 66192624.4731\n",
      "Fine-tune Pixel Refiner Epoch [185/500], Loss: 66184586.5376\n",
      "Fine-tune Pixel Refiner Epoch [186/500], Loss: 66265911.7204\n",
      "Fine-tune Pixel Refiner Epoch [187/500], Loss: 66181234.7957\n",
      "Fine-tune Pixel Refiner Epoch [188/500], Loss: 66224420.6237\n",
      "Fine-tune Pixel Refiner Epoch [189/500], Loss: 66179017.2473\n",
      "Fine-tune Pixel Refiner Epoch [190/500], Loss: 66187326.8817\n",
      "Fine-tune Pixel Refiner Epoch [191/500], Loss: 66263911.8925\n",
      "Fine-tune Pixel Refiner Epoch [192/500], Loss: 66234154.6237\n",
      "Fine-tune Pixel Refiner Epoch [193/500], Loss: 66293451.8710\n",
      "Fine-tune Pixel Refiner Epoch [194/500], Loss: 66170215.6237\n",
      "Fine-tune Pixel Refiner Epoch [195/500], Loss: 66165387.4839\n",
      "Fine-tune Pixel Refiner Epoch [196/500], Loss: 66243322.0108\n",
      "Fine-tune Pixel Refiner Epoch [197/500], Loss: 66160066.6452\n",
      "Fine-tune Pixel Refiner Epoch [198/500], Loss: 66182126.9462\n",
      "Fine-tune Pixel Refiner Epoch [199/500], Loss: 66147903.9140\n",
      "Fine-tune Pixel Refiner Epoch [200/500], Loss: 66173021.2581\n",
      "Fine-tune Pixel Refiner Epoch [201/500], Loss: 66177283.7204\n",
      "Fine-tune Pixel Refiner Epoch [202/500], Loss: 66342460.4086\n",
      "Fine-tune Pixel Refiner Epoch [203/500], Loss: 66139127.6989\n",
      "Fine-tune Pixel Refiner Epoch [204/500], Loss: 66235370.3226\n",
      "Fine-tune Pixel Refiner Epoch [205/500], Loss: 66181355.6344\n",
      "Fine-tune Pixel Refiner Epoch [206/500], Loss: 66160308.2366\n",
      "Fine-tune Pixel Refiner Epoch [207/500], Loss: 66237327.5376\n",
      "Fine-tune Pixel Refiner Epoch [208/500], Loss: 66153791.6989\n",
      "Fine-tune Pixel Refiner Epoch [209/500], Loss: 66138026.8817\n",
      "Fine-tune Pixel Refiner Epoch [210/500], Loss: 66136359.3978\n",
      "Fine-tune Pixel Refiner Epoch [211/500], Loss: 66174985.8280\n",
      "Fine-tune Pixel Refiner Epoch [212/500], Loss: 66112093.7849\n",
      "Fine-tune Pixel Refiner Epoch [213/500], Loss: 66188789.7419\n",
      "Fine-tune Pixel Refiner Epoch [214/500], Loss: 66242027.8065\n",
      "Fine-tune Pixel Refiner Epoch [215/500], Loss: 66140771.8280\n",
      "Fine-tune Pixel Refiner Epoch [216/500], Loss: 66146457.0323\n",
      "Fine-tune Pixel Refiner Epoch [217/500], Loss: 66123769.5054\n",
      "Fine-tune Pixel Refiner Epoch [218/500], Loss: 66102919.5806\n",
      "Fine-tune Pixel Refiner Epoch [219/500], Loss: 66169567.9140\n",
      "Fine-tune Pixel Refiner Epoch [220/500], Loss: 66108190.4516\n",
      "Fine-tune Pixel Refiner Epoch [221/500], Loss: 66112766.4301\n",
      "Fine-tune Pixel Refiner Epoch [222/500], Loss: 66129104.9032\n",
      "Fine-tune Pixel Refiner Epoch [223/500], Loss: 66091503.6774\n",
      "Fine-tune Pixel Refiner Epoch [224/500], Loss: 66077206.2366\n",
      "Fine-tune Pixel Refiner Epoch [225/500], Loss: 66122869.9140\n",
      "Fine-tune Pixel Refiner Epoch [226/500], Loss: 66178795.6774\n",
      "Fine-tune Pixel Refiner Epoch [227/500], Loss: 66210445.4194\n",
      "Fine-tune Pixel Refiner Epoch [228/500], Loss: 66057484.6022\n",
      "Fine-tune Pixel Refiner Epoch [229/500], Loss: 66136346.2366\n",
      "Fine-tune Pixel Refiner Epoch [230/500], Loss: 66110968.2151\n",
      "Fine-tune Pixel Refiner Epoch [231/500], Loss: 66139925.8495\n",
      "Fine-tune Pixel Refiner Epoch [232/500], Loss: 66077674.3011\n",
      "Fine-tune Pixel Refiner Epoch [233/500], Loss: 66104230.9247\n",
      "Fine-tune Pixel Refiner Epoch [234/500], Loss: 66099257.8065\n",
      "Fine-tune Pixel Refiner Epoch [235/500], Loss: 66044948.3871\n",
      "Fine-tune Pixel Refiner Epoch [236/500], Loss: 66106192.7957\n",
      "Fine-tune Pixel Refiner Epoch [237/500], Loss: 66091870.0860\n",
      "Fine-tune Pixel Refiner Epoch [238/500], Loss: 66176194.7742\n",
      "Fine-tune Pixel Refiner Epoch [239/500], Loss: 66097817.8280\n",
      "Fine-tune Pixel Refiner Epoch [240/500], Loss: 66031043.1398\n",
      "Fine-tune Pixel Refiner Epoch [241/500], Loss: 66292294.6452\n",
      "Fine-tune Pixel Refiner Epoch [242/500], Loss: 66076296.4946\n",
      "Fine-tune Pixel Refiner Epoch [243/500], Loss: 66088136.6237\n",
      "Fine-tune Pixel Refiner Epoch [244/500], Loss: 66025176.9892\n",
      "Fine-tune Pixel Refiner Epoch [245/500], Loss: 66141207.3763\n",
      "Fine-tune Pixel Refiner Epoch [246/500], Loss: 66051361.0538\n",
      "Fine-tune Pixel Refiner Epoch [247/500], Loss: 66025740.8817\n",
      "Fine-tune Pixel Refiner Epoch [248/500], Loss: 66237052.8817\n",
      "Fine-tune Pixel Refiner Epoch [249/500], Loss: 66074783.6344\n",
      "Fine-tune Pixel Refiner Epoch [250/500], Loss: 66030389.7634\n",
      "Fine-tune Pixel Refiner Epoch [251/500], Loss: 66122770.4516\n",
      "Fine-tune Pixel Refiner Epoch [252/500], Loss: 66130048.4946\n",
      "Fine-tune Pixel Refiner Epoch [253/500], Loss: 66072592.7204\n",
      "Fine-tune Pixel Refiner Epoch [254/500], Loss: 66014202.8602\n",
      "Fine-tune Pixel Refiner Epoch [255/500], Loss: 66026717.9570\n",
      "Fine-tune Pixel Refiner Epoch [256/500], Loss: 66000029.5699\n",
      "Fine-tune Pixel Refiner Epoch [257/500], Loss: 66072312.4731\n",
      "Fine-tune Pixel Refiner Epoch [258/500], Loss: 66006002.8817\n",
      "Fine-tune Pixel Refiner Epoch [259/500], Loss: 66040283.8280\n",
      "Fine-tune Pixel Refiner Epoch [260/500], Loss: 66013334.1505\n",
      "Fine-tune Pixel Refiner Epoch [261/500], Loss: 66013327.6559\n",
      "Fine-tune Pixel Refiner Epoch [262/500], Loss: 66059068.8172\n",
      "Fine-tune Pixel Refiner Epoch [263/500], Loss: 65971937.6559\n",
      "Fine-tune Pixel Refiner Epoch [264/500], Loss: 66035640.9462\n",
      "Fine-tune Pixel Refiner Epoch [265/500], Loss: 66031628.2366\n",
      "Fine-tune Pixel Refiner Epoch [266/500], Loss: 65986247.8495\n",
      "Fine-tune Pixel Refiner Epoch [267/500], Loss: 65995991.4624\n",
      "Fine-tune Pixel Refiner Epoch [268/500], Loss: 66025381.1398\n",
      "Fine-tune Pixel Refiner Epoch [269/500], Loss: 65991205.8925\n",
      "Fine-tune Pixel Refiner Epoch [270/500], Loss: 66005270.5591\n",
      "Fine-tune Pixel Refiner Epoch [271/500], Loss: 66021408.7957\n",
      "Fine-tune Pixel Refiner Epoch [272/500], Loss: 66011518.3011\n",
      "Fine-tune Pixel Refiner Epoch [273/500], Loss: 65995942.4731\n",
      "Fine-tune Pixel Refiner Epoch [274/500], Loss: 66064082.6022\n",
      "Fine-tune Pixel Refiner Epoch [275/500], Loss: 66004235.4839\n",
      "Fine-tune Pixel Refiner Epoch [276/500], Loss: 66083407.1613\n",
      "Fine-tune Pixel Refiner Epoch [277/500], Loss: 65939402.0215\n",
      "Fine-tune Pixel Refiner Epoch [278/500], Loss: 66075214.4086\n",
      "Fine-tune Pixel Refiner Epoch [279/500], Loss: 65963540.0000\n",
      "Fine-tune Pixel Refiner Epoch [280/500], Loss: 65985112.9462\n",
      "Fine-tune Pixel Refiner Epoch [281/500], Loss: 65971115.5269\n",
      "Fine-tune Pixel Refiner Epoch [282/500], Loss: 65949036.1290\n",
      "Fine-tune Pixel Refiner Epoch [283/500], Loss: 66058716.3763\n",
      "Fine-tune Pixel Refiner Epoch [284/500], Loss: 65967250.2366\n",
      "Fine-tune Pixel Refiner Epoch [285/500], Loss: 65993608.7957\n",
      "Fine-tune Pixel Refiner Epoch [286/500], Loss: 65963171.5269\n",
      "Fine-tune Pixel Refiner Epoch [287/500], Loss: 65954273.0108\n",
      "Fine-tune Pixel Refiner Epoch [288/500], Loss: 65944498.4731\n",
      "Fine-tune Pixel Refiner Epoch [289/500], Loss: 66013294.9892\n",
      "Fine-tune Pixel Refiner Epoch [290/500], Loss: 66022793.7634\n",
      "Fine-tune Pixel Refiner Epoch [291/500], Loss: 65945189.6774\n",
      "Fine-tune Pixel Refiner Epoch [292/500], Loss: 65912419.7204\n",
      "Fine-tune Pixel Refiner Epoch [293/500], Loss: 65904229.3333\n",
      "Fine-tune Pixel Refiner Epoch [294/500], Loss: 65908394.1720\n",
      "Fine-tune Pixel Refiner Epoch [295/500], Loss: 65931155.0323\n",
      "Fine-tune Pixel Refiner Epoch [296/500], Loss: 65905781.7957\n",
      "Fine-tune Pixel Refiner Epoch [297/500], Loss: 65924249.3656\n",
      "Fine-tune Pixel Refiner Epoch [298/500], Loss: 66041107.0323\n",
      "Fine-tune Pixel Refiner Epoch [299/500], Loss: 65948610.8495\n",
      "Fine-tune Pixel Refiner Epoch [300/500], Loss: 65912722.6882\n",
      "Fine-tune Pixel Refiner Epoch [301/500], Loss: 65956786.3656\n",
      "Fine-tune Pixel Refiner Epoch [302/500], Loss: 66073296.1720\n",
      "Fine-tune Pixel Refiner Epoch [303/500], Loss: 65891325.0108\n",
      "Fine-tune Pixel Refiner Epoch [304/500], Loss: 65889493.1398\n",
      "Fine-tune Pixel Refiner Epoch [305/500], Loss: 65931072.6452\n",
      "Fine-tune Pixel Refiner Epoch [306/500], Loss: 65884170.0645\n",
      "Fine-tune Pixel Refiner Epoch [307/500], Loss: 65900556.6237\n",
      "Fine-tune Pixel Refiner Epoch [308/500], Loss: 66033953.7849\n",
      "Fine-tune Pixel Refiner Epoch [309/500], Loss: 65911029.8495\n",
      "Fine-tune Pixel Refiner Epoch [310/500], Loss: 65932735.7527\n",
      "Fine-tune Pixel Refiner Epoch [311/500], Loss: 65871321.2903\n",
      "Fine-tune Pixel Refiner Epoch [312/500], Loss: 65909387.0323\n",
      "Fine-tune Pixel Refiner Epoch [313/500], Loss: 65927080.8172\n",
      "Fine-tune Pixel Refiner Epoch [314/500], Loss: 65895598.9247\n",
      "Fine-tune Pixel Refiner Epoch [315/500], Loss: 65866378.4946\n",
      "Fine-tune Pixel Refiner Epoch [316/500], Loss: 65970539.2473\n",
      "Fine-tune Pixel Refiner Epoch [317/500], Loss: 65891650.5161\n",
      "Fine-tune Pixel Refiner Epoch [318/500], Loss: 65848118.6452\n",
      "Fine-tune Pixel Refiner Epoch [319/500], Loss: 65869379.6774\n",
      "Fine-tune Pixel Refiner Epoch [320/500], Loss: 65989636.9032\n",
      "Fine-tune Pixel Refiner Epoch [321/500], Loss: 65867355.3441\n",
      "Fine-tune Pixel Refiner Epoch [322/500], Loss: 65949756.7097\n",
      "Fine-tune Pixel Refiner Epoch [323/500], Loss: 65855149.4301\n",
      "Fine-tune Pixel Refiner Epoch [324/500], Loss: 65996950.9892\n",
      "Fine-tune Pixel Refiner Epoch [325/500], Loss: 65924169.8710\n",
      "Fine-tune Pixel Refiner Epoch [326/500], Loss: 65864525.1183\n",
      "Fine-tune Pixel Refiner Epoch [327/500], Loss: 65863685.6344\n",
      "Fine-tune Pixel Refiner Epoch [328/500], Loss: 65835226.2366\n",
      "Fine-tune Pixel Refiner Epoch [329/500], Loss: 65822656.2366\n",
      "Fine-tune Pixel Refiner Epoch [330/500], Loss: 65865185.2258\n",
      "Fine-tune Pixel Refiner Epoch [331/500], Loss: 65827831.0753\n",
      "Fine-tune Pixel Refiner Epoch [332/500], Loss: 65838455.9570\n",
      "Fine-tune Pixel Refiner Epoch [333/500], Loss: 65865668.0645\n",
      "Fine-tune Pixel Refiner Epoch [334/500], Loss: 65830007.2473\n",
      "Fine-tune Pixel Refiner Epoch [335/500], Loss: 66013157.8925\n",
      "Fine-tune Pixel Refiner Epoch [336/500], Loss: 65876100.7742\n",
      "Fine-tune Pixel Refiner Epoch [337/500], Loss: 65827757.8495\n",
      "Fine-tune Pixel Refiner Epoch [338/500], Loss: 65846418.5161\n",
      "Fine-tune Pixel Refiner Epoch [339/500], Loss: 65971168.8172\n",
      "Fine-tune Pixel Refiner Epoch [340/500], Loss: 65813870.6667\n",
      "Fine-tune Pixel Refiner Epoch [341/500], Loss: 65787497.0968\n",
      "Fine-tune Pixel Refiner Epoch [342/500], Loss: 65880846.4946\n",
      "Fine-tune Pixel Refiner Epoch [343/500], Loss: 65919039.4409\n",
      "Fine-tune Pixel Refiner Epoch [344/500], Loss: 65795393.9140\n",
      "Fine-tune Pixel Refiner Epoch [345/500], Loss: 65850282.8817\n",
      "Fine-tune Pixel Refiner Epoch [346/500], Loss: 65776912.5591\n",
      "Fine-tune Pixel Refiner Epoch [347/500], Loss: 65786073.7419\n",
      "Fine-tune Pixel Refiner Epoch [348/500], Loss: 65799517.5484\n",
      "Fine-tune Pixel Refiner Epoch [349/500], Loss: 65816454.4301\n",
      "Fine-tune Pixel Refiner Epoch [350/500], Loss: 65768367.8925\n",
      "Fine-tune Pixel Refiner Epoch [351/500], Loss: 65780448.8172\n",
      "Fine-tune Pixel Refiner Epoch [352/500], Loss: 65886590.8602\n",
      "Fine-tune Pixel Refiner Epoch [353/500], Loss: 65757687.0323\n",
      "Fine-tune Pixel Refiner Epoch [354/500], Loss: 65793192.4946\n",
      "Fine-tune Pixel Refiner Epoch [355/500], Loss: 65805858.6882\n",
      "Fine-tune Pixel Refiner Epoch [356/500], Loss: 65823925.1828\n",
      "Fine-tune Pixel Refiner Epoch [357/500], Loss: 65761354.0000\n",
      "Fine-tune Pixel Refiner Epoch [358/500], Loss: 65843915.1398\n",
      "Fine-tune Pixel Refiner Epoch [359/500], Loss: 65748366.2796\n",
      "Fine-tune Pixel Refiner Epoch [360/500], Loss: 65748301.0323\n",
      "Fine-tune Pixel Refiner Epoch [361/500], Loss: 65760814.3441\n",
      "Fine-tune Pixel Refiner Epoch [362/500], Loss: 65766283.6344\n",
      "Fine-tune Pixel Refiner Epoch [363/500], Loss: 66003473.2473\n",
      "Fine-tune Pixel Refiner Epoch [364/500], Loss: 65769710.7527\n",
      "Fine-tune Pixel Refiner Epoch [365/500], Loss: 65764010.6667\n",
      "Fine-tune Pixel Refiner Epoch [366/500], Loss: 65783888.6667\n",
      "Fine-tune Pixel Refiner Epoch [367/500], Loss: 65807423.5484\n",
      "Fine-tune Pixel Refiner Epoch [368/500], Loss: 65745325.3978\n",
      "Fine-tune Pixel Refiner Epoch [369/500], Loss: 65746363.0968\n",
      "Fine-tune Pixel Refiner Epoch [370/500], Loss: 65744494.9462\n",
      "Fine-tune Pixel Refiner Epoch [371/500], Loss: 65731798.6452\n",
      "Fine-tune Pixel Refiner Epoch [372/500], Loss: 65864745.8925\n",
      "Fine-tune Pixel Refiner Epoch [373/500], Loss: 65884428.0860\n",
      "Fine-tune Pixel Refiner Epoch [374/500], Loss: 65790450.0215\n",
      "Fine-tune Pixel Refiner Epoch [375/500], Loss: 65830980.7742\n",
      "Fine-tune Pixel Refiner Epoch [376/500], Loss: 65718744.3871\n",
      "Fine-tune Pixel Refiner Epoch [377/500], Loss: 65765979.1828\n",
      "Fine-tune Pixel Refiner Epoch [378/500], Loss: 65709101.2258\n",
      "Fine-tune Pixel Refiner Epoch [379/500], Loss: 65717688.3441\n",
      "Fine-tune Pixel Refiner Epoch [380/500], Loss: 65746230.7097\n",
      "Fine-tune Pixel Refiner Epoch [381/500], Loss: 65756527.2258\n",
      "Fine-tune Pixel Refiner Epoch [382/500], Loss: 65782141.3763\n",
      "Fine-tune Pixel Refiner Epoch [383/500], Loss: 65939720.6022\n",
      "Fine-tune Pixel Refiner Epoch [384/500], Loss: 65771822.5914\n",
      "Fine-tune Pixel Refiner Epoch [385/500], Loss: 65685141.8495\n",
      "Fine-tune Pixel Refiner Epoch [386/500], Loss: 65749198.9032\n",
      "Fine-tune Pixel Refiner Epoch [387/500], Loss: 65724573.7957\n",
      "Fine-tune Pixel Refiner Epoch [388/500], Loss: 65699751.6344\n",
      "Fine-tune Pixel Refiner Epoch [389/500], Loss: 65680552.3226\n",
      "Fine-tune Pixel Refiner Epoch [390/500], Loss: 65681579.1613\n",
      "Fine-tune Pixel Refiner Epoch [391/500], Loss: 65717238.6882\n",
      "Fine-tune Pixel Refiner Epoch [392/500], Loss: 65702145.0968\n",
      "Fine-tune Pixel Refiner Epoch [393/500], Loss: 65725726.3656\n",
      "Fine-tune Pixel Refiner Epoch [394/500], Loss: 65715699.1828\n",
      "Fine-tune Pixel Refiner Epoch [395/500], Loss: 65714996.4731\n",
      "Fine-tune Pixel Refiner Epoch [396/500], Loss: 65726388.5806\n",
      "Fine-tune Pixel Refiner Epoch [397/500], Loss: 65724646.3871\n",
      "Fine-tune Pixel Refiner Epoch [398/500], Loss: 65786159.7204\n",
      "Fine-tune Pixel Refiner Epoch [399/500], Loss: 65689849.4409\n",
      "Fine-tune Pixel Refiner Epoch [400/500], Loss: 65642754.3441\n",
      "Fine-tune Pixel Refiner Epoch [401/500], Loss: 65676679.7204\n",
      "Fine-tune Pixel Refiner Epoch [402/500], Loss: 65746928.0860\n",
      "Fine-tune Pixel Refiner Epoch [403/500], Loss: 65682269.6774\n",
      "Fine-tune Pixel Refiner Epoch [404/500], Loss: 65632367.4946\n",
      "Fine-tune Pixel Refiner Epoch [405/500], Loss: 65706520.4516\n",
      "Fine-tune Pixel Refiner Epoch [406/500], Loss: 65649172.9462\n",
      "Fine-tune Pixel Refiner Epoch [407/500], Loss: 65662481.7634\n",
      "Fine-tune Pixel Refiner Epoch [408/500], Loss: 65718910.9462\n",
      "Fine-tune Pixel Refiner Epoch [409/500], Loss: 65670129.3118\n",
      "Fine-tune Pixel Refiner Epoch [410/500], Loss: 65651450.7312\n",
      "Fine-tune Pixel Refiner Epoch [411/500], Loss: 65721648.6882\n",
      "Fine-tune Pixel Refiner Epoch [412/500], Loss: 65904882.8387\n",
      "Fine-tune Pixel Refiner Epoch [413/500], Loss: 65682590.7742\n",
      "Fine-tune Pixel Refiner Epoch [414/500], Loss: 65650751.9140\n",
      "Fine-tune Pixel Refiner Epoch [415/500], Loss: 65792630.9247\n",
      "Fine-tune Pixel Refiner Epoch [416/500], Loss: 65616197.3978\n",
      "Fine-tune Pixel Refiner Epoch [417/500], Loss: 65666262.1075\n",
      "Fine-tune Pixel Refiner Epoch [418/500], Loss: 65622616.4946\n",
      "Fine-tune Pixel Refiner Epoch [419/500], Loss: 65637418.0215\n",
      "Fine-tune Pixel Refiner Epoch [420/500], Loss: 65725754.7957\n",
      "Fine-tune Pixel Refiner Epoch [421/500], Loss: 65626483.2151\n",
      "Fine-tune Pixel Refiner Epoch [422/500], Loss: 65615243.9140\n",
      "Fine-tune Pixel Refiner Epoch [423/500], Loss: 65745397.9140\n",
      "Fine-tune Pixel Refiner Epoch [424/500], Loss: 65625919.5591\n",
      "Fine-tune Pixel Refiner Epoch [425/500], Loss: 65662179.0323\n",
      "Fine-tune Pixel Refiner Epoch [426/500], Loss: 65727923.7419\n",
      "Fine-tune Pixel Refiner Epoch [427/500], Loss: 65643001.5054\n",
      "Fine-tune Pixel Refiner Epoch [428/500], Loss: 65633932.7312\n",
      "Fine-tune Pixel Refiner Epoch [429/500], Loss: 65603104.1398\n",
      "Fine-tune Pixel Refiner Epoch [430/500], Loss: 65642879.5699\n",
      "Fine-tune Pixel Refiner Epoch [431/500], Loss: 65655870.5591\n",
      "Fine-tune Pixel Refiner Epoch [432/500], Loss: 65666854.2366\n",
      "Fine-tune Pixel Refiner Epoch [433/500], Loss: 65624649.8710\n",
      "Fine-tune Pixel Refiner Epoch [434/500], Loss: 65574886.6882\n",
      "Fine-tune Pixel Refiner Epoch [435/500], Loss: 65657995.6774\n",
      "Fine-tune Pixel Refiner Epoch [436/500], Loss: 65680657.1935\n",
      "Fine-tune Pixel Refiner Epoch [437/500], Loss: 65754398.9462\n",
      "Fine-tune Pixel Refiner Epoch [438/500], Loss: 65580609.7849\n",
      "Fine-tune Pixel Refiner Epoch [439/500], Loss: 65629907.0645\n",
      "Fine-tune Pixel Refiner Epoch [440/500], Loss: 65618784.9677\n",
      "Fine-tune Pixel Refiner Epoch [441/500], Loss: 65639862.0430\n",
      "Fine-tune Pixel Refiner Epoch [442/500], Loss: 65564082.6882\n",
      "Fine-tune Pixel Refiner Epoch [443/500], Loss: 65586282.8602\n",
      "Fine-tune Pixel Refiner Epoch [444/500], Loss: 65612678.2581\n",
      "Fine-tune Pixel Refiner Epoch [445/500], Loss: 65631596.4946\n",
      "Fine-tune Pixel Refiner Epoch [446/500], Loss: 65720408.1935\n",
      "Fine-tune Pixel Refiner Epoch [447/500], Loss: 65574462.0215\n",
      "Fine-tune Pixel Refiner Epoch [448/500], Loss: 65548904.1935\n",
      "Fine-tune Pixel Refiner Epoch [449/500], Loss: 65581859.1183\n",
      "Fine-tune Pixel Refiner Epoch [450/500], Loss: 65548095.4194\n",
      "Fine-tune Pixel Refiner Epoch [451/500], Loss: 65611664.2366\n",
      "Fine-tune Pixel Refiner Epoch [452/500], Loss: 65583028.6237\n",
      "Fine-tune Pixel Refiner Epoch [453/500], Loss: 65615665.1398\n",
      "Fine-tune Pixel Refiner Epoch [454/500], Loss: 65607632.8172\n",
      "Fine-tune Pixel Refiner Epoch [455/500], Loss: 65593428.0430\n",
      "Fine-tune Pixel Refiner Epoch [456/500], Loss: 65585156.2796\n",
      "Fine-tune Pixel Refiner Epoch [457/500], Loss: 65558701.9570\n",
      "Fine-tune Pixel Refiner Epoch [458/500], Loss: 65560026.8925\n",
      "Fine-tune Pixel Refiner Epoch [459/500], Loss: 65612758.9677\n",
      "Fine-tune Pixel Refiner Epoch [460/500], Loss: 65600535.0538\n",
      "Fine-tune Pixel Refiner Epoch [461/500], Loss: 65603501.6989\n",
      "Fine-tune Pixel Refiner Epoch [462/500], Loss: 65600365.5484\n",
      "Fine-tune Pixel Refiner Epoch [463/500], Loss: 65577006.6237\n",
      "Fine-tune Pixel Refiner Epoch [464/500], Loss: 65565740.3226\n",
      "Fine-tune Pixel Refiner Epoch [465/500], Loss: 65533801.1828\n",
      "Fine-tune Pixel Refiner Epoch [466/500], Loss: 65603643.6344\n",
      "Fine-tune Pixel Refiner Epoch [467/500], Loss: 65545065.5914\n",
      "Fine-tune Pixel Refiner Epoch [468/500], Loss: 65571253.0753\n",
      "Fine-tune Pixel Refiner Epoch [469/500], Loss: 65523610.0645\n",
      "Fine-tune Pixel Refiner Epoch [470/500], Loss: 65510128.8387\n",
      "Fine-tune Pixel Refiner Epoch [471/500], Loss: 65509279.1613\n",
      "Fine-tune Pixel Refiner Epoch [472/500], Loss: 65507104.2151\n",
      "Fine-tune Pixel Refiner Epoch [473/500], Loss: 65616864.2581\n",
      "Fine-tune Pixel Refiner Epoch [474/500], Loss: 65550971.3763\n",
      "Fine-tune Pixel Refiner Epoch [475/500], Loss: 65497354.2151\n",
      "Fine-tune Pixel Refiner Epoch [476/500], Loss: 65511036.8602\n",
      "Fine-tune Pixel Refiner Epoch [477/500], Loss: 65496466.4516\n",
      "Fine-tune Pixel Refiner Epoch [478/500], Loss: 65497105.8495\n",
      "Fine-tune Pixel Refiner Epoch [479/500], Loss: 65492852.4946\n",
      "Fine-tune Pixel Refiner Epoch [480/500], Loss: 65536254.5591\n",
      "Fine-tune Pixel Refiner Epoch [481/500], Loss: 65540070.5161\n",
      "Fine-tune Pixel Refiner Epoch [482/500], Loss: 65513458.8172\n",
      "Fine-tune Pixel Refiner Epoch [483/500], Loss: 65623474.7097\n",
      "Fine-tune Pixel Refiner Epoch [484/500], Loss: 65499194.0108\n",
      "Fine-tune Pixel Refiner Epoch [485/500], Loss: 65503599.2043\n",
      "Fine-tune Pixel Refiner Epoch [486/500], Loss: 65576883.9785\n",
      "Fine-tune Pixel Refiner Epoch [487/500], Loss: 65496417.2151\n",
      "Fine-tune Pixel Refiner Epoch [488/500], Loss: 65525981.5269\n",
      "Fine-tune Pixel Refiner Epoch [489/500], Loss: 65500376.7957\n",
      "Fine-tune Pixel Refiner Epoch [490/500], Loss: 65463326.3656\n",
      "Fine-tune Pixel Refiner Epoch [491/500], Loss: 65499770.8172\n",
      "Fine-tune Pixel Refiner Epoch [492/500], Loss: 65481944.9462\n",
      "Fine-tune Pixel Refiner Epoch [493/500], Loss: 65505399.4839\n",
      "Fine-tune Pixel Refiner Epoch [494/500], Loss: 65463777.1183\n",
      "Fine-tune Pixel Refiner Epoch [495/500], Loss: 65537667.0968\n",
      "Fine-tune Pixel Refiner Epoch [496/500], Loss: 65495412.7742\n",
      "Fine-tune Pixel Refiner Epoch [497/500], Loss: 65558702.3226\n",
      "Fine-tune Pixel Refiner Epoch [498/500], Loss: 65474149.9785\n",
      "Fine-tune Pixel Refiner Epoch [499/500], Loss: 65469581.9570\n",
      "Fine-tune Pixel Refiner Epoch [500/500], Loss: 65563767.5699\n",
      "Pixel Refiner fine-tuned and saved successfully!\n"
     ]
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T03:11:50.025180Z",
     "start_time": "2025-03-30T03:11:43.885519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load existing Pixel Refiner clearly\n",
    "refiner_model.load_state_dict(torch.load('pixel_refiner_continued.pth', map_location=device))\n",
    "refiner_model.train()\n",
    "\n",
    "criterion_refiner = nn.MSELoss()\n",
    "optimizer_refiner = torch.optim.Adam(refiner_model.parameters(), lr=1e-6)\n",
    "\n",
    "fine_tune_epochs = 10\n",
    "for epoch in range(fine_tune_epochs):\n",
    "    total_loss = 0\n",
    "    for imgs, initial_preds, gt_pixels in updated_loader:\n",
    "        imgs, initial_preds, gt_pixels = imgs.to(device), initial_preds.to(device), gt_pixels.to(device)\n",
    "\n",
    "        optimizer_refiner.zero_grad()\n",
    "        refined_output = refiner_model(imgs, initial_preds)\n",
    "        loss = criterion_refiner(refined_output, gt_pixels)\n",
    "        loss.backward()\n",
    "        optimizer_refiner.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(updated_loader)\n",
    "    print(f\"Fine-tune Pixel Refiner Epoch [{epoch+1}/{fine_tune_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "torch.save(refiner_model.state_dict(), 'pixel_refiner_finetuned.pth')\n",
    "print(\"Pixel Refiner fine-tuned and saved!\")\n"
   ],
   "id": "f750d26fc52ebd1c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Pixel Refiner Epoch [1/10], Loss: 178960180.2151\n",
      "Fine-tune Pixel Refiner Epoch [2/10], Loss: 178873978.5376\n",
      "Fine-tune Pixel Refiner Epoch [3/10], Loss: 178702388.3011\n",
      "Fine-tune Pixel Refiner Epoch [4/10], Loss: 178503169.2043\n",
      "Fine-tune Pixel Refiner Epoch [5/10], Loss: 178424672.1290\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[80], line 17\u001B[0m\n\u001B[1;32m     15\u001B[0m refined_output \u001B[38;5;241m=\u001B[39m refiner_model(imgs, initial_preds)\n\u001B[1;32m     16\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion_refiner(refined_output, gt_pixels)\n\u001B[0;32m---> 17\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     18\u001B[0m optimizer_refiner\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     20\u001B[0m total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[0;32m~/PyCharmMiscProject/.venv/lib/python3.9/site-packages/torch/_tensor.py:626\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    616\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    617\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    618\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    619\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    624\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    625\u001B[0m     )\n\u001B[0;32m--> 626\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    627\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    628\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PyCharmMiscProject/.venv/lib/python3.9/site-packages/torch/autograd/__init__.py:347\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    342\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    344\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    345\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    346\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 347\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    348\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    349\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    350\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    351\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    352\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    353\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    355\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PyCharmMiscProject/.venv/lib/python3.9/site-packages/torch/autograd/graph.py:823\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    821\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[1;32m    822\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 823\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    824\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    825\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    826\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    827\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T03:38:01.534088Z",
     "start_time": "2025-03-30T03:38:01.519003Z"
    }
   },
   "cell_type": "code",
   "source": "yolo detect predict model=runs/detect/train/weights/best.pt source=./test_dataset/test_dataset/ save_txt save_conf\n",
   "id": "72410757f4f98f98",
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3995509010.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  Cell \u001B[0;32mIn[83], line 1\u001B[0;36m\u001B[0m\n\u001B[0;31m    yolo detect predict model=runs/detect/train/weights/best.pt source=./test_dataset/test_dataset/ save_txt save_conf\u001B[0m\n\u001B[0m         ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "execution_count": 83
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
